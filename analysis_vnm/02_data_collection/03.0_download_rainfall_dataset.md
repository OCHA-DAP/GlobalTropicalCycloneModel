# Notebook that downloads GPM rainfall data done per typhoon


```python
import getpass
import os
from pathlib import Path

import pandas as pd
import datetime as dt
from bs4 import BeautifulSoup
import requests
import time
```


```python
# Setting directories
input_dir = (
    Path(os.getenv("STORM_DATA_DIR"))
    / "analysis_vnm/02_model_features/03_rainfall/input"
)
# Setting path to save the GPM data
gpm_file_name = "gpm_data/rainfall_data/output_hhr/"
gpm_folder_path = Path(input_dir, gpm_file_name)
```


```python
# To create an account for downloading the data
# follow the instructions here: https://registration.pps.eosdis.nasa.gov/registration/
# Change the user name and provide the password in the code
# Normally, is just the passwords and the user is just the email that you registered.
USERNAME =  getpass.getpass(prompt="Username: ", stream=None)
PASSWORD = getpass.getpass(prompt="Password: ", stream=None)

# Setting the number of days prior to the landfall data for which to collect data
DAYS_TO_LANDFALL = 2
```


```python
# Load and clean the typhoon metadata
# We really only care about the landfall date
typhoon_metadata = pd.read_csv(input_dir / "metadata_typhoons.csv").set_index(
    "typhoon"
)
for colname in ["startdate", "enddate", "landfalldate"]:
    typhoon_metadata[colname] = pd.to_datetime(
        typhoon_metadata[colname], format="%Y-%m-%d"
    )
typhoon_metadata
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>startdate</th>
      <th>enddate</th>
      <th>landfalldate</th>
      <th>landfall_time</th>
    </tr>
    <tr>
      <th>typhoon</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>LINDA1997</th>
      <td>1997-11-01</td>
      <td>1997-11-09</td>
      <td>1997-11-02</td>
      <td>10:00:00</td>
    </tr>
    <tr>
      <th>DAMREY2005</th>
      <td>2005-09-21</td>
      <td>2005-09-27</td>
      <td>2005-09-27</td>
      <td>03:00:00</td>
    </tr>
    <tr>
      <th>XANGSANE2006</th>
      <td>2006-09-26</td>
      <td>2006-10-01</td>
      <td>2006-10-01</td>
      <td>02:00:00</td>
    </tr>
    <tr>
      <th>DURIAN2006</th>
      <td>2006-11-26</td>
      <td>2006-12-05</td>
      <td>2006-12-05</td>
      <td>00:00:00</td>
    </tr>
    <tr>
      <th>LEKIMA2007</th>
      <td>2007-09-30</td>
      <td>2007-10-04</td>
      <td>2007-10-03</td>
      <td>13:00:00</td>
    </tr>
    <tr>
      <th>KAMMURI2008</th>
      <td>2008-08-05</td>
      <td>2008-08-07</td>
      <td>2008-08-07</td>
      <td>09:00:00</td>
    </tr>
    <tr>
      <th>KETSANA2009</th>
      <td>2009-09-26</td>
      <td>2009-09-30</td>
      <td>2009-09-29</td>
      <td>07:00:00</td>
    </tr>
    <tr>
      <th>CONSON2010</th>
      <td>2010-07-12</td>
      <td>2010-07-17</td>
      <td>2010-07-17</td>
      <td>14:00:00</td>
    </tr>
    <tr>
      <th>MINDULLE2010</th>
      <td>2010-08-23</td>
      <td>2010-08-24</td>
      <td>2010-08-24</td>
      <td>11:00:00</td>
    </tr>
    <tr>
      <th>HAIMA2011</th>
      <td>2011-06-21</td>
      <td>2011-06-24</td>
      <td>2011-06-24</td>
      <td>11:00:00</td>
    </tr>
    <tr>
      <th>NESAT2011</th>
      <td>2011-09-24</td>
      <td>2011-09-30</td>
      <td>2011-09-30</td>
      <td>06:00:00</td>
    </tr>
    <tr>
      <th>PAKHAR2012</th>
      <td>2012-03-29</td>
      <td>2012-04-01</td>
      <td>2012-04-01</td>
      <td>12:00:00</td>
    </tr>
    <tr>
      <th>VICENTE2012</th>
      <td>2012-07-21</td>
      <td>2012-07-24</td>
      <td>2012-07-24</td>
      <td>12:00:00</td>
    </tr>
    <tr>
      <th>GAEMI2012</th>
      <td>2012-10-01</td>
      <td>2012-10-06</td>
      <td>2012-10-06</td>
      <td>06:00:00</td>
    </tr>
    <tr>
      <th>BEBINCA2013</th>
      <td>2013-06-20</td>
      <td>2013-06-24</td>
      <td>2013-06-23</td>
      <td>12:00:00</td>
    </tr>
    <tr>
      <th>JEBI2013</th>
      <td>2013-07-31</td>
      <td>2013-08-03</td>
      <td>2013-08-03</td>
      <td>02:00:00</td>
    </tr>
    <tr>
      <th>MANGKHUT2013</th>
      <td>2013-08-06</td>
      <td>2013-08-07</td>
      <td>2013-08-07</td>
      <td>16:00:00</td>
    </tr>
    <tr>
      <th>WUTIP2013</th>
      <td>2013-09-27</td>
      <td>2013-09-30</td>
      <td>2013-09-29</td>
      <td>02:00:00</td>
    </tr>
    <tr>
      <th>NARI2013</th>
      <td>2013-10-09</td>
      <td>2013-10-15</td>
      <td>2013-10-14</td>
      <td>23:00:00</td>
    </tr>
    <tr>
      <th>RAMMASUN2014</th>
      <td>2014-07-12</td>
      <td>2014-07-19</td>
      <td>2014-07-19</td>
      <td>10:00:00</td>
    </tr>
    <tr>
      <th>KALMAEGI2014</th>
      <td>2014-09-12</td>
      <td>2014-09-17</td>
      <td>2014-09-16</td>
      <td>14:00:00</td>
    </tr>
    <tr>
      <th>KUJIRA2015</th>
      <td>2015-06-21</td>
      <td>2015-06-24</td>
      <td>2015-06-24</td>
      <td>07:00:00</td>
    </tr>
    <tr>
      <th>MIRINAE2016</th>
      <td>2016-07-26</td>
      <td>2016-07-28</td>
      <td>2016-07-27</td>
      <td>17:00:00</td>
    </tr>
    <tr>
      <th>DIANMU2016</th>
      <td>2016-08-17</td>
      <td>2016-08-19</td>
      <td>2016-08-19</td>
      <td>05:00:00</td>
    </tr>
    <tr>
      <th>RAI2016</th>
      <td>2016-09-12</td>
      <td>2016-09-13</td>
      <td>2016-09-12</td>
      <td>20:00:00</td>
    </tr>
    <tr>
      <th>TALAS2017</th>
      <td>2017-07-15</td>
      <td>2017-07-17</td>
      <td>2017-07-16</td>
      <td>17:00:00</td>
    </tr>
    <tr>
      <th>SONCA2017</th>
      <td>2017-07-23</td>
      <td>2017-07-25</td>
      <td>2017-07-25</td>
      <td>06:00:00</td>
    </tr>
    <tr>
      <th>DOKSURI2017</th>
      <td>2017-09-12</td>
      <td>2017-09-15</td>
      <td>2017-09-14</td>
      <td>04:00:00</td>
    </tr>
    <tr>
      <th>DAMREY2017</th>
      <td>2017-11-02</td>
      <td>2017-11-04</td>
      <td>2017-11-04</td>
      <td>00:00:00</td>
    </tr>
    <tr>
      <th>WIPHA2019</th>
      <td>2019-07-30</td>
      <td>2019-08-03</td>
      <td>2019-08-02</td>
      <td>15:00:00</td>
    </tr>
    <tr>
      <th>PODUL2019</th>
      <td>2019-08-28</td>
      <td>2019-08-29</td>
      <td>2019-08-29</td>
      <td>17:00:00</td>
    </tr>
  </tbody>
</table>
</div>




```python
#%% Functions used
def list_files(url):
    page = requests.get(url, auth=(USERNAME, PASSWORD)).text
    soup = BeautifulSoup(page, "html.parser")
    return [
        url + "/" + node.get("href")
        for node in soup.find_all("a")
        if node.get("href").endswith("tif")
    ]


def download_gpm_http(start_date, end_date, download_path):
    base_url = "https://arthurhouhttps.pps.eosdis.nasa.gov/pub/gpmdata"

    date_list = pd.date_range(start_date, end_date)
    file_list = []

    for date in date_list:
        #print(f"Downloading data for date {date}")
        day_path = download_path / date.strftime("%Y%m%d")
        day_path.mkdir(parents=True, exist_ok=True)

        url = f"{base_url}/{date.strftime('%Y/%m/%d')}/gis"
        tiff_files = list_files(url=url)

        for tiff_file in tiff_files:
            file_name = tiff_file.split("/")[-1]

            file_path = day_path / file_name
            file_list.append(file_path)
            r = requests.get(tiff_file, auth=(USERNAME, USERNAME))
            time.sleep(0.2)
            open(file_path, "wb").write(r.content)

    return file_list
```

## Download the data

This section is for downloading the data.
It takes a long time to complete.


```python
i=0 # Sometimes if there's a problem, just identify the typhoon number and start downloading again from that.
for typhoon, metadata in typhoon_metadata[i:].iterrows():
    start_date = metadata["landfalldate"] - dt.timedelta(days=DAYS_TO_LANDFALL)
    end_date = metadata["landfalldate"]# + dt.timedelta(days=DAYS_TO_LANDFALL)
    print('Typhoon {}/{}'.format(i, len(typhoon_metadata)-1));i+=1
    print(f"Downloading data for {typhoon} between {start_date} and {end_date}")
    download_gpm_http(start_date=start_date,
                      end_date=end_date,
                      download_path=gpm_folder_path / typhoon / "GPM")
```
