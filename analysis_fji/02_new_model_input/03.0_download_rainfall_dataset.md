# Notebook that downloads GPM rainfall data done per typhoon


```python
import getpass
import os
from pathlib import Path


import pandas as pd
import datetime as dt
from bs4 import BeautifulSoup
import requests
import time
```


```python
# Setting directories
input_dir = (
    Path(os.getenv("STORM_DATA_DIR"))
    / "analysis_fji/02_new_model_input/03_rainfall/input"
)
# Setting path to save the GPM data
gpm_file_name = "gpm_data/rainfall_data/output_hhr/"
gpm_folder_path = Path(input_dir, gpm_file_name)
```


```python
gpm_folder_path
```




    PosixPath('/Users/federico/Documents/data/analysis_fji/02_new_model_input/03_rainfall/input/gpm_data/rainfall_data/output_hhr')




```python
# To create an account for downloading the data
# follow the instructions here: https://registration.pps.eosdis.nasa.gov/registration/
# Change the user name and provide the password in the code
USERNAME =  getpass.getpass(prompt="Username: ", stream=None)
PASSWORD = getpass.getpass(prompt="Password: ", stream=None)

# Setting the number of days prior to the landfall data for which to collect data
DAYS_TO_LANDFALL = 2
```

    Username:  ········
    Password:  ········



```python
# Load and clean the typhoon metadata
# We really only care about the landfall date
typhoon_metadata = pd.read_csv(input_dir / "metadata_typhoons.csv").set_index(
    "typhoon"
)
for colname in ["startdate", "enddate", "landfalldate"]:
    typhoon_metadata[colname] = pd.to_datetime(
        typhoon_metadata[colname], format="%d/%m/%Y"
    )
typhoon_metadata
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>startdate</th>
      <th>enddate</th>
      <th>landfalldate</th>
      <th>landfall_time</th>
    </tr>
    <tr>
      <th>typhoon</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>bebe1972</th>
      <td>1972-10-19</td>
      <td>1972-10-26</td>
      <td>1972-10-22</td>
      <td>20:30:00</td>
    </tr>
    <tr>
      <th>bebe1972</th>
      <td>1972-10-27</td>
      <td>1972-10-28</td>
      <td>1972-10-27</td>
      <td>00:00:00</td>
    </tr>
    <tr>
      <th>juliette1973</th>
      <td>1973-04-03</td>
      <td>1973-04-04</td>
      <td>1973-04-03</td>
      <td>12:30:00</td>
    </tr>
    <tr>
      <th>juliette1973</th>
      <td>1973-04-02</td>
      <td>1973-04-03</td>
      <td>1973-04-03</td>
      <td>00:00:00</td>
    </tr>
    <tr>
      <th>juliette1973</th>
      <td>1973-04-05</td>
      <td>1973-04-06</td>
      <td>1973-04-05</td>
      <td>00:00:00</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>tino2020</th>
      <td>2020-01-16</td>
      <td>2020-01-19</td>
      <td>2020-01-17</td>
      <td>05:30:00</td>
    </tr>
    <tr>
      <th>harold2020</th>
      <td>2020-04-01</td>
      <td>2020-04-10</td>
      <td>2020-04-08</td>
      <td>00:30:00</td>
    </tr>
    <tr>
      <th>yasa2020</th>
      <td>2020-12-13</td>
      <td>2020-12-20</td>
      <td>2020-12-17</td>
      <td>06:00:00</td>
    </tr>
    <tr>
      <th>bina2021</th>
      <td>2021-01-31</td>
      <td>2021-01-31</td>
      <td>2021-01-31</td>
      <td>13:30:00</td>
    </tr>
    <tr>
      <th>ana2021</th>
      <td>2021-01-29</td>
      <td>2021-02-01</td>
      <td>2021-01-30</td>
      <td>10:00:00</td>
    </tr>
  </tbody>
</table>
<p>77 rows × 4 columns</p>
</div>




```python
#%% Functions used
def list_files(url):
    page = requests.get(url, auth=(USERNAME, PASSWORD)).text
    soup = BeautifulSoup(page, "html.parser")
    return [
        url + "/" + node.get("href")
        for node in soup.find_all("a")
        if node.get("href").endswith("tif")
    ]


def download_gpm_http(start_date, end_date, download_path):
    base_url = "https://arthurhouhttps.pps.eosdis.nasa.gov/pub/gpmdata"

    date_list = pd.date_range(start_date, end_date)
    file_list = []

    for date in date_list:
        #print(f"Downloading data for date {date}")
        day_path = download_path / date.strftime("%Y%m%d")
        day_path.mkdir(parents=True, exist_ok=True)

        url = f"{base_url}/{date.strftime('%Y/%m/%d')}/gis"
        tiff_files = list_files(url=url)

        for tiff_file in tiff_files:
            file_name = tiff_file.split("/")[-1]

            file_path = day_path / file_name
            file_list.append(file_path)
            r = requests.get(tiff_file, auth=(USERNAME, USERNAME))
            time.sleep(0.2)
            open(file_path, "wb").write(r.content)

    return file_list
```

## Download the data

This section is for downloading the data.
It takes a long time to complete.


```python
i=0 # Sometimes if there's a problem, just identify the typhoon number and start downloading again from that.
for typhoon, metadata in typhoon_metadata[i:].iterrows():
    start_date = metadata["landfalldate"] - dt.timedelta(days=DAYS_TO_LANDFALL)
    end_date = metadata["landfalldate"] - dt.timedelta(days=DAYS_TO_LANDFALL)
    print('Typhoon {}/{}'.format(i, len(typhoon_metadata)-1));i+=1
    print(f"Downloading data for {typhoon} between {start_date} and {end_date}")
    download_gpm_http(start_date=start_date,
                      end_date=end_date,
                      download_path=gpm_folder_path / typhoon / "GPM")
```

    Typhoon 58/77
    Downloading data for wilma2011 between 2011-01-23 00:00:00 and 2011-01-23 00:00:00
    Typhoon 59/77
    Downloading data for atu2011 between 2011-02-15 00:00:00 and 2011-02-15 00:00:00
    Typhoon 60/77
    Downloading data for bune2011 between 2011-03-21 00:00:00 and 2011-03-21 00:00:00
    Typhoon 61/77
    Downloading data for daphne2012 between 2012-03-31 00:00:00 and 2012-03-31 00:00:00
    Typhoon 62/77
    Downloading data for evan2012 between 2012-12-15 00:00:00 and 2012-12-15 00:00:00
    Typhoon 63/77
    Downloading data for winston2016 between 2016-02-17 00:00:00 and 2016-02-17 00:00:00
    Typhoon 64/77
    Downloading data for zena2016 between 2016-04-04 00:00:00 and 2016-04-04 00:00:00
    Typhoon 65/77
    Downloading data for ella2017 between 2017-05-12 00:00:00 and 2017-05-12 00:00:00
    Typhoon 66/77
    Downloading data for gita2018 between 2018-02-11 00:00:00 and 2018-02-11 00:00:00
    Typhoon 67/77
    Downloading data for josie2018 between 2018-03-30 00:00:00 and 2018-03-30 00:00:00
    Typhoon 68/77
    Downloading data for keni2018 between 2018-04-08 00:00:00 and 2018-04-08 00:00:00
    Typhoon 69/77
    Downloading data for mona2019 between 2019-01-05 00:00:00 and 2019-01-05 00:00:00
    Typhoon 70/77
    Downloading data for pola2019 between 2019-02-25 00:00:00 and 2019-02-25 00:00:00
    Typhoon 71/77
    Downloading data for sarai2019 between 2019-12-27 00:00:00 and 2019-12-27 00:00:00
    Typhoon 72/77
    Downloading data for tino2020 between 2020-01-15 00:00:00 and 2020-01-15 00:00:00
    Typhoon 73/77
    Downloading data for harold2020 between 2020-04-06 00:00:00 and 2020-04-06 00:00:00
    Typhoon 74/77
    Downloading data for yasa2020 between 2020-12-15 00:00:00 and 2020-12-15 00:00:00
    Typhoon 75/77
    Downloading data for bina2021 between 2021-01-29 00:00:00 and 2021-01-29 00:00:00
    Typhoon 76/77
    Downloading data for ana2021 between 2021-01-28 00:00:00 and 2021-01-28 00:00:00
