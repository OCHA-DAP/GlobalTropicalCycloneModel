---
jupyter:
  jupytext:
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: env6
    language: python
    name: python3
---

```python
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
import matplotlib.patches as mpatches
import numpy as np
import os
from pathlib import Path
import pandas as pd
# import shap
from sklearn.metrics import mean_squared_error
from xgboost.sklearn import XGBRegressor

from utils import get_combined_dataset, xgb_model_combined_data_LOOCV, get_training_dataset_complete, get_municipality_grids
```

```python
# Load dataset
df_combined = get_combined_dataset()
df_fji = df_combined[df_combined.country == 'fji']

# Typhoons
fji_typhoons = df_fji.typhoon_name.unique()

# Features
features = [
    "wind_speed",
    "track_distance",
    "total_houses",
    "rainfall_max_6h",
    "rainfall_max_24h",
    "coast_length",
    "with_coast",
    "mean_elev",
    "mean_slope",
]
```

```python
# Stratification
dmg = np.array(df_fji.percent_houses_damaged.to_list())
zero_dmg = np.round((np.count_nonzero(dmg == 0) / len(dmg)) , 2 )

# Define ranges for each group
x0 = list(np.linspace(0, zero_dmg, 1))   # zero damage
x1 = list(np.linspace(zero_dmg, 0.93, 2))  # almost no damage
x2 = list(np.linspace(0.935, 1, 5))  # all the damage
x3=x0+x1+x2

bins = []
for i in x3:
    bins.append(np.quantile(dmg, i))

# Histogram after stratification
samples_per_bin_fji, bins_def_fji = np.histogram(dmg, bins=bins)

# Define number of bins
num_bins_fji = len(samples_per_bin_fji)

# For future plots
str_bin_fji = []
for i in range(len(bins_def_fji[:-1])):
    a = str(np.round(bins_def_fji[i+1],3))
    b = str(np.round(bins_def_fji[i],3))
    str_bin_fji.append('{} - {}'.format(b,a))
```

```python
# Drop ANA typhoon
df_combined_noana = df_combined[df_combined.typhoon_name != 'ANA']
df_fji_noana= df_fji[df_fji.typhoon_name != 'ANA']

# Run XGB
y_test_typhoon, y_pred_typhoon, rmse_strat, avg_error_strat = xgb_model_combined_data_LOOCV(
    df_combined=df_combined_noana,
    df_fji=df_fji_noana,
    bins=bins_def_fji,
    fji_weight=3,
    features=features
)
```

```python
""" THE MODEL """
bins=bins_def_fji
fji_weight=6

df_fji = df_fji_noana.copy()
df_combined = df_combined_noana.copy()

# Dataframe Fiji
fji_typhoons = df_fji.typhoon_name.unique()

# Bins
num_bins = len(bins)

# The model
rmse_total_fji = []
rmse_bin_fji = []
avg_error_bin_fji = []

y_test_typhoon_fji  = []
y_pred_typhoon_fji  = []
# Create an empty dictionary to store feature importances for each typhoon (each LOOCV iteration)
feature_importance_dict = {feature: [] for feature in features}
YASA_fi = []

for typhoon in fji_typhoons:

    """ PART 1: Train/Test """

    # LOOCV
    df_test = df_fji[df_fji["typhoon_name"] == typhoon] # Test set: Fiji
    df_train = df_combined[df_combined["typhoon_name"] != typhoon] # Train set: everything

    # Class weight
    weights = np.where(df_train['country'] == 'phl', 1, fji_weight) # Let's give more weight to Fiji

    # Split X and y from dataframe features
    X_test = df_test[features]
    X_train = df_train[features]

    y_train = df_train["percent_houses_damaged"]
    y_test = df_test["percent_houses_damaged"]

    # Stratify data
    bin_index_test = np.digitize(y_test, bins=bins[:-1])

    """ PART 2: XGB regressor """
    # create an XGBoost Regressor
    xgb = XGBRegressor(
        base_score=0.5,
        booster="gbtree",
        colsample_bylevel=0.8,
        colsample_bynode=0.8,
        colsample_bytree=0.8,
        gamma=3,
        eta=0.01,
        importance_type="gain",
        learning_rate=0.1,
        max_delta_step=0,
        max_depth=4,
        min_child_weight=1,
        missing=1,
        n_estimators=100,
        early_stopping_rounds=10,
        n_jobs=1,
        nthread=None,
        objective="reg:squarederror",
        reg_alpha=0,
        reg_lambda=1,
        scale_pos_weight=1,
        seed=None,
        silent=None,
        subsample=0.8,
        verbosity=0,
        eval_metric=["rmse", "logloss"],
        random_state=0,
    )

    # Fit the model
    eval_set = [(X_train, y_train)]
    xgb.fit(X_train, y_train, eval_set=eval_set, verbose=False, sample_weight=weights) #xgb_model

    # Get feature importance and append to dictionary
    feature_importance = xgb.feature_importances_
    for idx, feature in enumerate(features):
        feature_importance_dict[feature].append(feature_importance[idx])

    if typhoon == "YASA":
        YASA_fi.append(feature_importance)

    # make predictions on Fiji
    y_pred_fji = xgb.predict(X_test)

    # Save y_test y_pred
    y_test_typhoon_fji.append(y_test)
    y_pred_typhoon_fji.append(y_pred_fji)

    # Calculate root mean squared error in total
    mse_test = mean_squared_error(y_test, y_pred_fji)
    rmse_test = np.sqrt(mse_test)
    rmse_total_fji.append(rmse_test)

    # Per bin (Stratification)
    rmse_test_bin = []
    avg_error_bin = []
    for bin_num in range(num_bins)[1:]:
        if (len(y_test[bin_index_test == bin_num]) != 0 and len(y_pred_fji[bin_index_test == bin_num]) != 0):
            # Estimation of RMSE for test data per each bin
            mse_test = mean_squared_error(y_test[bin_index_test == bin_num], y_pred_fji[bin_index_test == bin_num])
            rmse_test = np.sqrt(mse_test)
            rmse_test_bin.append(rmse_test)
            # Avg error
            mean_difference = np.mean(y_test[bin_index_test == bin_num] - y_pred_fji[bin_index_test == bin_num])
            avg_error_bin.append(mean_difference)
        else:
            rmse_test_bin.append(np.nan)
            avg_error_bin.append(np.nan)

    rmse_bin_fji.append(rmse_test_bin)
    avg_error_bin_fji.append(avg_error_bin)

# RMSE & Avg error per bin
rmse_strat_fji = []
avg_error_strat_fji = []
for i in range(num_bins - 1):
    #RMSE
    test_rmse_bin = np.nanmean(np.array(rmse_bin_fji)[:,i])
    rmse_strat_fji.append(test_rmse_bin)
    #AVG error
    test_avg_bin = np.nanmean(np.array(avg_error_bin_fji)[:,i])
    avg_error_strat_fji.append(test_avg_bin)

```

```python
""" Feature importance """
# Calculate mean importance across all iterations
mean_importance = {feature: np.mean(importances) for feature, importances in feature_importance_dict.items()}

# Sort feature importance in descending order
sorted_importance = {k: v for k, v in sorted(mean_importance.items(), key=lambda item: item[1], reverse=False)}

# Plot
plt.figure(figsize=(5, 5))
sorted_values = list(sorted_importance.values())
sorted_keys = list(sorted_importance.keys())
plt.barh(range(len(sorted_importance)), sorted_values, align="center", color='skyblue')
plt.yticks(range(len(sorted_importance)), sorted_keys)
plt.xlabel("Mean Importance")
plt.title("Feature Importance \n(XGB built-in feature)")
plt.tight_layout()
plt.show()
```

### Special case: YASA

```python
yasa_importance = pd.DataFrame({'feature': features,
              'importance': YASA_fi[0]}).sort_values('importance')
yasa_importance
```

```python

plt.barh(range(len(yasa_importance)), yasa_importance.importance, align="center", color='skyblue')
plt.yticks(range(len(yasa_importance)), yasa_importance.feature)
plt.xlabel("Importance")
plt.title("Feature Importance \n(XGB built-in feature) \n YASA typhoon")
plt.tight_layout()
plt.show()
```
