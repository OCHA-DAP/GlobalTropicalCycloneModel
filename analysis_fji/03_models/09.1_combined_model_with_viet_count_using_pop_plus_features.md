```python
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
import matplotlib.patches as mpatches
import numpy as np
import os
from pathlib import Path
import pandas as pd
import geopandas as gpd
from climada.hazard import Centroids, TCTracks, TropCyclone
from shapely.geometry import LineString
from sklearn.metrics import mean_squared_error
from xgboost.sklearn import XGBRegressor
import geopandas as gpd
from climada.hazard import Centroids, TCTracks, TropCyclone
from shapely.geometry import LineString
from xgboost import plot_importance
import shap

from utils import get_combined_dataset_interpolated, get_combined_dataset_interpolated_with_viet, get_combined_dataset_interpolated_with_viet_new_bld_count, get_combined_dataset_interpolated_with_viet_new_bld_count_using_pop,  get_training_dataset_complete_interpolated, get_training_dataset_complete_interpolated_new_bld_count, get_training_dataset_complete_interpolated_new_bld_count_using_pop , get_municipality_grids, xgb_model_combined_data_LOOCV, xgb_model_combined_data_with_viet_LOOCV
```

    IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html


## Loading data


```python
# For Checking (Number of buildings destroyed per mun)
input_dir = (
    Path(os.getenv("STORM_DATA_DIR"))
    / "analysis_fji/02_model_features/02_housing_damage/input/"
)
actual_mun_dmg = pd.read_csv(input_dir / "fji_building_damage_mun_complete.csv")
actual_mun_dmg['typhoon'] = actual_mun_dmg['typhoon'].str.upper()

# Load Fiji Shapefile
fiji = gpd.read_file(
    input_dir / "adm2_shp_fixed.gpkg"
)
fiji = fiji.to_crs('EPSG:4326')

# Load grid
grid_dir = (
    Path(os.getenv("STORM_DATA_DIR"))
    / "analysis_fji/02_model_features/02_housing_damage/output/"
)
grid = gpd.read_file(grid_dir / "fji_0.1_degree_grid_land_overlap_new.gpkg")


# Load typhoon track
tracks_dir = (
    Path(os.getenv("STORM_DATA_DIR"))
    / "analysis_fji/02_model_features/01_windfield"
)
typhoons_info = pd.read_csv(tracks_dir / "typhoons.csv")
typhoons_info.typhoon_name = typhoons_info.typhoon_name.str.upper()
cyclones = actual_mun_dmg['typhoon'].unique()
intersection = typhoons_info[typhoons_info['typhoon_name'].isin(cyclones)].drop_duplicates(keep='last', subset = ['typhoon_name'])
```


```python
# Features
features_old = [
    "wind_speed",
    "track_distance",
    "total_houses",
    "rainfall_max_6h",
    "rainfall_max_24h",
    "coast_length",
    "with_coast",
    "mean_elev",
    "mean_slope",
]
features_IWI = features_old + ["IWI"]
features_light = features_old + ["light_index"]
features_all = features_old + ["IWI", "light_index"]
```


```python
# Load Fiji complete interpolated new 2 (using pop data to bld count)
df_fji_complete_int = get_training_dataset_complete_interpolated_new_bld_count_using_pop()
df_fji_complete_int = df_fji_complete_int[df_fji_complete_int.typhoon_name != 'ANA'] # Drop ANA
df_fji_complete_int = df_fji_complete_int.rename({
    "mean_altitude": "mean_elev",
    "total_buildings": "total_houses",
    "perc_dmg_grid": "percent_houses_damaged"
}, axis=1)
df_fji_complete_int = df_fji_complete_int[features_all + ['typhoon_name', 'grid_point_id', 'Centroid', 'percent_houses_damaged']]

# Load combined dataset interpolated
df_combined_with_viet = get_combined_dataset_interpolated_with_viet_new_bld_count_using_pop()
df_fji_with_viet = df_combined_with_viet[df_combined_with_viet.country == 'fji']
```

## Stratification


```python
fig, ax = plt.subplots(1,2, figsize=(10,6))

ax[0].hist(df_fji_complete_int.percent_houses_damaged, edgecolor='black')
ax[0].set_yscale('log')
ax[0].set_xlabel('% Houses Damaged',size=15)
ax[0].set_ylabel('Frequency',size=15)

hist = np.histogram(df_fji_complete_int.percent_houses_damaged, bins=10 ** np.linspace(0, np.log10(10**1.5), len(df_fji_complete_int)), density=True)
x = hist[1][:-1]
y = hist[0]

ax[1].plot(x,y, 'ro', alpha=0.4, label='housing damage')
ax[1].set_xscale('log')
ax[1].set_yscale('log')
ax[1].set_xlabel('% Houses Damaged', size=15)
ax[1].set_ylabel('Frequency', size=15) #PDF (% Houses Damaged)
ax[1].grid(c='black', alpha=0.3)
ax[1].legend()

plt.tight_layout()
plt.show()
```



![png](09.1_combined_model_with_viet_count_using_pop_plus_features_files/09.1_combined_model_with_viet_count_using_pop_plus_features_6_0.png)




```python
#Lets look for the perfect binning
dmg = np.array(df_fji_complete_int.percent_houses_damaged.to_list())
offset = 1e-8
dmg_off = dmg + offset
x = list(np.linspace(0,1,101))
info = []
for i in x:
    info.append(np.quantile(dmg_off, i))

plt.plot(x,info, 'o')
plt.xlabel('Quantile')
plt.ylabel('Damage [%]')
plt.yscale('log')
plt.title('Damage Offset = {}'.format(offset))
plt.grid()
plt.show()
```



![png](09.1_combined_model_with_viet_count_using_pop_plus_features_files/09.1_combined_model_with_viet_count_using_pop_plus_features_7_0.png)




```python
# Stratification
dmg = np.array(df_fji_with_viet.percent_houses_damaged.to_list())
zero_dmg = np.round((np.count_nonzero(dmg == 0) / len(dmg)) , 2 )

# Define ranges for each group
x0 = list(np.linspace(0, zero_dmg, 1))   # zero damage
x1 = list(np.linspace(zero_dmg, 0.9, 2))  # almost no damage
x2 = list(np.linspace(0.935, 1, 5))  # all the damage
x3=x0+x1+x2

bins = []
for i in x3:
    bins.append(np.quantile(dmg, i))

# Histogram after stratification
samples_per_bin_fji, bins_def_fji = np.histogram(dmg, bins=bins)

# Define number of bins
num_bins_fji = len(samples_per_bin_fji)

# For future plots
str_bin_fji = []
for i in range(len(bins_def_fji[:-1])):
    a = str(np.round(bins_def_fji[i+1],3))
    b = str(np.round(bins_def_fji[i],3))
    str_bin_fji.append('{} - {}'.format(b,a))
```

## The models


```python
# Fji + Phl + Viet
y_test_typhoon_with_viet, y_pred_typhoon_with_viet, rmse_strat_with_viet, avg_error_strat_with_viet = xgb_model_combined_data_with_viet_LOOCV(
    df_combined=df_combined_with_viet,
    df_fji=df_fji_with_viet,
    bins=bins_def_fji,
    fji_weight=3,
    features=features_old
)
```

    /Users/federico/Library/CloudStorage/OneDrive-Personal/Documentos/ISI_Project/FIJI clean/analysis_fji/03_models/utils.py:570: RuntimeWarning: Mean of empty slice
      test_rmse_bin = np.nanmean(np.array(rmse_bin_fji)[:,i])
    /Users/federico/Library/CloudStorage/OneDrive-Personal/Documentos/ISI_Project/FIJI clean/analysis_fji/03_models/utils.py:573: RuntimeWarning: Mean of empty slice
      test_avg_bin = np.nanmean(np.array(avg_error_bin_fji)[:,i])



```python
# Fji + Phl + Viet (with IWI)
y_test_typhoon_with_viet_iwi, y_pred_typhoon_with_viet_iwi, rmse_strat_with_viet_iwi, avg_error_strat_with_viet_iwi = xgb_model_combined_data_with_viet_LOOCV(
    df_combined=df_combined_with_viet,
    df_fji=df_fji_with_viet,
    bins=bins_def_fji,
    fji_weight=3,
    features=features_IWI
)
```

    /Users/federico/Library/CloudStorage/OneDrive-Personal/Documentos/ISI_Project/FIJI clean/analysis_fji/03_models/utils.py:570: RuntimeWarning: Mean of empty slice
      test_rmse_bin = np.nanmean(np.array(rmse_bin_fji)[:,i])
    /Users/federico/Library/CloudStorage/OneDrive-Personal/Documentos/ISI_Project/FIJI clean/analysis_fji/03_models/utils.py:573: RuntimeWarning: Mean of empty slice
      test_avg_bin = np.nanmean(np.array(avg_error_bin_fji)[:,i])



```python
# Fji + Phl + Viet (with Light Index)
y_test_typhoon_with_viet_light, y_pred_typhoon_with_viet_light, rmse_strat_with_viet_light, avg_error_strat_with_viet_light = xgb_model_combined_data_with_viet_LOOCV(
    df_combined=df_combined_with_viet,
    df_fji=df_fji_with_viet,
    bins=bins_def_fji,
    fji_weight=3,
    features=features_light
)
```

    /Users/federico/Library/CloudStorage/OneDrive-Personal/Documentos/ISI_Project/FIJI clean/analysis_fji/03_models/utils.py:570: RuntimeWarning: Mean of empty slice
      test_rmse_bin = np.nanmean(np.array(rmse_bin_fji)[:,i])
    /Users/federico/Library/CloudStorage/OneDrive-Personal/Documentos/ISI_Project/FIJI clean/analysis_fji/03_models/utils.py:573: RuntimeWarning: Mean of empty slice
      test_avg_bin = np.nanmean(np.array(avg_error_bin_fji)[:,i])



```python
# Fji + Phl + Viet (with Light Index + IWI)
y_test_typhoon_with_viet_all, y_pred_typhoon_with_viet_all, rmse_strat_with_viet_all, avg_error_strat_with_viet_all = xgb_model_combined_data_with_viet_LOOCV(
    df_combined=df_combined_with_viet,
    df_fji=df_fji_with_viet,
    bins=bins_def_fji,
    fji_weight=3,
    features=features_all
)
```

    /Users/federico/Library/CloudStorage/OneDrive-Personal/Documentos/ISI_Project/FIJI clean/analysis_fji/03_models/utils.py:570: RuntimeWarning: Mean of empty slice
      test_rmse_bin = np.nanmean(np.array(rmse_bin_fji)[:,i])
    /Users/federico/Library/CloudStorage/OneDrive-Personal/Documentos/ISI_Project/FIJI clean/analysis_fji/03_models/utils.py:573: RuntimeWarning: Mean of empty slice
      test_avg_bin = np.nanmean(np.array(avg_error_bin_fji)[:,i])


## Results per bin


```python
fig, ax = plt.subplots(1,1, figsize=(6,6))

ax.plot(range(num_bins_fji)[1:], rmse_strat_with_viet[1:], 'rs', alpha=0.5, label='Classic')
ax.plot(range(num_bins_fji)[1:], rmse_strat_with_viet_iwi[1:], 'bs', alpha=0.5, label='+ IWI feature')
ax.plot(range(num_bins_fji)[1:], rmse_strat_with_viet_light[1:], 'gs', alpha=0.5, label='+ Light Index feature')
ax.plot(range(num_bins_fji)[1:], rmse_strat_with_viet_all[1:], 'ks', alpha=0.5, label='+ IWI + Light Index features')

ax.set_xticks(range(num_bins_fji)[1:], str_bin_fji[1:], rotation=45)
ax.set_xlabel('Damage [%]')
ax.set_ylabel('RMSE')
ax.grid()
ax.set_title('XGBoost Regression model (PHL+VIET+FJI) \n using LOOCV for Fiji typhoons')
ax.legend()

plt.tight_layout()
plt.show()
```



![png](09.1_combined_model_with_viet_count_using_pop_plus_features_files/09.1_combined_model_with_viet_count_using_pop_plus_features_15_0.png)



## Results at municipality level


```python
# Fiji typhoons
fji_typhoons = df_fji_with_viet.typhoon_name.unique()

# Calculate buildings destroyed by municipality and % of buildings destroyed by mun
mun_id = get_municipality_grids()[['id','NAME_2']]

def num_bld_destroyed_mun(mun, typhoon, y_pred_typhoon, df_fji, real=False):
    k = fji_typhoons.tolist().index(typhoon)
    df_typhoon = df_fji[df_fji.typhoon_name==typhoon]
    # Add feature "predictive_damage"
    df_typhoon['predicted_damage'] = y_pred_typhoon[k]

    mun_ids = mun_id[mun_id.NAME_2 == mun].id.to_list()
    cells_in_mun = df_typhoon[df_typhoon.typhoon_name == typhoon].set_index('grid_point_id').loc[mun_ids]

    if real:
        damage_grid = np.array(cells_in_mun.percent_houses_damaged.to_list()) # Real dmg
    else:
        damage_grid = np.array(cells_in_mun.predicted_damage.to_list()) # Dmg predicted by cell

    # Number of buildings
    N_bld_grid = np.array(cells_in_mun.total_houses.to_list()) # Bld by cell
    N_bld_mun = np.sum(N_bld_grid) # Total bld in mun

    # Calculate % of buildings (and N of bld) destroyed by mun
    N_bld_dest_pred_mun = np.sum(damage_grid) * (N_bld_mun / 100)
    perc_destroyed_mun = np.sum(damage_grid)

    return N_bld_dest_pred_mun, perc_destroyed_mun

def calculate_actual_perc_dmg(x, i, y_pred_typhoon, df_fji, typhoon):
    try:
        return num_bld_destroyed_mun(mun=x['NAME_2'], typhoon=typhoon, y_pred_typhoon=y_pred_typhoon, df_fji=df_fji, real=True)[i]
    except:
        return 0
def calculate_pred_perc_dmg(x, i, y_pred_typhoon, df_fji, typhoon):
    try:
        return num_bld_destroyed_mun(mun=x['NAME_2'], typhoon=typhoon, y_pred_typhoon=y_pred_typhoon, df_fji=df_fji, real=False)[i]
    except:
        return 0
```

### General results


```python
from sklearn.metrics import mean_squared_error

typhoons = df_fji_with_viet.typhoon_name.unique()
rmse_viet_tot = []
rmse_viet_new_tot = []
rmse_viet_new2_tot = []
rmse_viet_new3_tot = []
for typhoon in typhoons:

    # Classic approach PHL+FJI+VIET
    fiji_old = fiji.copy()
    fiji_old['actual_perc_dmg'] = fiji_old.apply(calculate_actual_perc_dmg, df_fji=df_fji_complete_int,
                                                y_pred_typhoon=y_pred_typhoon_with_viet, i=1, typhoon=typhoon, axis=1)
    fiji_old['pred_perc_dmg'] = fiji_old.apply(calculate_pred_perc_dmg, df_fji=df_fji_complete_int,
                                            y_pred_typhoon=y_pred_typhoon_with_viet, i=1, typhoon=typhoon, axis=1)

    # Modify values
    fiji_old['actual_perc_dmg'] = fiji_old['actual_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
    fiji_old['pred_perc_dmg'] = fiji_old['pred_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
    # Now calculate prediciton error
    fiji_old['prediction_error'] = fiji_old['actual_perc_dmg'] - fiji_old['pred_perc_dmg'] # in percentual points

    # Classic approach PHL+FJI+VIET + IWI feature
    fiji_iwi = fiji.copy()
    fiji_iwi['actual_perc_dmg'] = fiji_iwi.apply(calculate_actual_perc_dmg, df_fji=df_fji_complete_int,
                                                y_pred_typhoon=y_pred_typhoon_with_viet_iwi, i=1, typhoon=typhoon, axis=1)
    fiji_iwi['pred_perc_dmg'] = fiji_iwi.apply(calculate_pred_perc_dmg, df_fji=df_fji_complete_int,
                                            y_pred_typhoon=y_pred_typhoon_with_viet_iwi, i=1, typhoon=typhoon, axis=1)

    # Modify values
    fiji_iwi['actual_perc_dmg'] = fiji_iwi['actual_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
    fiji_iwi['pred_perc_dmg'] = fiji_iwi['pred_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
    # Now calculate prediciton error
    fiji_iwi['prediction_error'] = fiji_iwi['actual_perc_dmg'] - fiji_iwi['pred_perc_dmg'] # in percentual points

    # Classic approach PHL+FJI+VIET + light feature
    fiji_light = fiji.copy()
    fiji_light['actual_perc_dmg'] = fiji_light.apply(calculate_actual_perc_dmg, df_fji=df_fji_complete_int,
                                                y_pred_typhoon=y_pred_typhoon_with_viet_light, i=1, typhoon=typhoon, axis=1)
    fiji_light['pred_perc_dmg'] = fiji_light.apply(calculate_pred_perc_dmg, df_fji=df_fji_complete_int,
                                            y_pred_typhoon=y_pred_typhoon_with_viet_light, i=1, typhoon=typhoon, axis=1)

    # Modify values
    fiji_light['actual_perc_dmg'] = fiji_light['actual_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
    fiji_light['pred_perc_dmg'] = fiji_light['pred_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
    # Now calculate prediciton error
    fiji_light['prediction_error'] = fiji_light['actual_perc_dmg'] - fiji_light['pred_perc_dmg'] # in percentual points

    # Classic approach PHL+FJI+VIET + IWI AND light features
    fiji_all = fiji.copy()
    fiji_all['actual_perc_dmg'] = fiji_all.apply(calculate_actual_perc_dmg, df_fji=df_fji_complete_int,
                                                y_pred_typhoon=y_pred_typhoon_with_viet_all, i=1, typhoon=typhoon, axis=1)
    fiji_all['pred_perc_dmg'] = fiji_all.apply(calculate_pred_perc_dmg, df_fji=df_fji_complete_int,
                                            y_pred_typhoon=y_pred_typhoon_with_viet_all, i=1, typhoon=typhoon, axis=1)

    # Modify values
    fiji_all['actual_perc_dmg'] = fiji_all['actual_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
    fiji_all['pred_perc_dmg'] = fiji_all['pred_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
    # Now calculate prediciton error
    fiji_all['prediction_error'] = fiji_all['actual_perc_dmg'] - fiji_all['pred_perc_dmg'] # in percentual points


    actual_dmg_old = fiji_old['actual_perc_dmg']
    actual_dmg_new = fiji_iwi['actual_perc_dmg']
    actual_dmg_new2 = fiji_light['actual_perc_dmg']
    actual_dmg_new3 = fiji_all['actual_perc_dmg']
    pred_dmg_viet = fiji_old['pred_perc_dmg']
    pred_dmg_viet_new = fiji_iwi['pred_perc_dmg']
    pred_dmg_viet_new2 = fiji_light['pred_perc_dmg']
    pred_dmg_viet_new3 = fiji_all['pred_perc_dmg']
    muns = fiji_old['NAME_2']

    rmse_viet = []
    rmse_viet_new = []
    rmse_viet_new2 = []
    rmse_viet_new3 = []
    for i in range(len(muns)):
        rmse_viet.append(np.sqrt(mean_squared_error([actual_dmg_old[i]], [pred_dmg_viet[i]])))
        rmse_viet_new.append(np.sqrt(mean_squared_error([actual_dmg_new[i]], [pred_dmg_viet_new[i]])))
        rmse_viet_new2.append(np.sqrt(mean_squared_error([actual_dmg_new2[i]], [pred_dmg_viet_new2[i]])))
        rmse_viet_new3.append(np.sqrt(mean_squared_error([actual_dmg_new3[i]], [pred_dmg_viet_new3[i]])))

    rmse_viet_tot.append(rmse_viet)
    rmse_viet_new_tot.append(rmse_viet_new)
    rmse_viet_new2_tot.append(rmse_viet_new2)
    rmse_viet_new3_tot.append(rmse_viet_new3)
```


```python
# RMSE per mun
rmse_mun_fji_viet = []
rmse_mun_fji_viet_new = []
rmse_mun_fji_viet_new2 = []
rmse_mun_fji_viet_new3 = []
for i in range(len(muns)):
    #RMSE classic
    test_rmse_mun = np.nanmean(np.array(rmse_viet_tot)[:,i])
    rmse_mun_fji_viet.append(test_rmse_mun)

    #RMSE iwi
    test_rmse_mun_viet = np.nanmean(np.array(rmse_viet_new_tot)[:,i])
    rmse_mun_fji_viet_new.append(test_rmse_mun_viet)

    #RMSE lights
    test_rmse_mun_viet2 = np.nanmean(np.array(rmse_viet_new2_tot)[:,i])
    rmse_mun_fji_viet_new2.append(test_rmse_mun_viet2)

    #RMSE lights and IWI
    test_rmse_mun_viet3 = np.nanmean(np.array(rmse_viet_new3_tot)[:,i])
    rmse_mun_fji_viet_new3.append(test_rmse_mun_viet3)
```


```python
plt.plot(rmse_mun_fji_viet, 'o-', label='Classic features', alpha=0.8)
plt.plot(rmse_mun_fji_viet_new, 'o-', label='+ IWI feature', alpha=0.6)
plt.plot(rmse_mun_fji_viet_new2, 'o-', label='+ Light Index feature', alpha=0.6)
plt.plot(rmse_mun_fji_viet_new3, 'o-', label='+ IWI + Light Index feature', alpha=0.6)

plt.xticks(range(len(muns)), muns, rotation='vertical')  # Set x-axis ticks and labels
plt.tight_layout()  # Adjust layout to prevent overlapping
plt.title('Tropical Cyclones, Fiji \nPHL+FJI+VIET model')
plt.ylabel('RMSE')

plt.legend()
plt.show()
```



![png](09.1_combined_model_with_viet_count_using_pop_plus_features_files/09.1_combined_model_with_viet_count_using_pop_plus_features_21_0.png)



### Specific cases


```python
typhoon='GITA'
```


```python
# Classic approach PHL+FJI+VIET
fiji_old = fiji.copy()
fiji_old['actual_perc_dmg'] = fiji_old.apply(calculate_actual_perc_dmg, df_fji=df_fji_complete_int,
                                            y_pred_typhoon=y_pred_typhoon_with_viet, i=1, typhoon=typhoon, axis=1)
fiji_old['pred_perc_dmg'] = fiji_old.apply(calculate_pred_perc_dmg, df_fji=df_fji_complete_int,
                                        y_pred_typhoon=y_pred_typhoon_with_viet, i=1, typhoon=typhoon, axis=1)

# Modify values
fiji_old['actual_perc_dmg'] = fiji_old['actual_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
fiji_old['pred_perc_dmg'] = fiji_old['pred_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
# Now calculate prediciton error
fiji_old['prediction_error'] = fiji_old['actual_perc_dmg'] - fiji_old['pred_perc_dmg'] # in percentual points

# Classic approach PHL+FJI+VIET + IWI feature
fiji_iwi = fiji.copy()
fiji_iwi['actual_perc_dmg'] = fiji_iwi.apply(calculate_actual_perc_dmg, df_fji=df_fji_complete_int,
                                            y_pred_typhoon=y_pred_typhoon_with_viet_iwi, i=1, typhoon=typhoon, axis=1)
fiji_iwi['pred_perc_dmg'] = fiji_iwi.apply(calculate_pred_perc_dmg, df_fji=df_fji_complete_int,
                                        y_pred_typhoon=y_pred_typhoon_with_viet_iwi, i=1, typhoon=typhoon, axis=1)

# Modify values
fiji_iwi['actual_perc_dmg'] = fiji_iwi['actual_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
fiji_iwi['pred_perc_dmg'] = fiji_iwi['pred_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
# Now calculate prediciton error
fiji_iwi['prediction_error'] = fiji_iwi['actual_perc_dmg'] - fiji_iwi['pred_perc_dmg'] # in percentual points

# Classic approach PHL+FJI+VIET + light feature
fiji_light = fiji.copy()
fiji_light['actual_perc_dmg'] = fiji_light.apply(calculate_actual_perc_dmg, df_fji=df_fji_complete_int,
                                            y_pred_typhoon=y_pred_typhoon_with_viet_light, i=1, typhoon=typhoon, axis=1)
fiji_light['pred_perc_dmg'] = fiji_light.apply(calculate_pred_perc_dmg, df_fji=df_fji_complete_int,
                                        y_pred_typhoon=y_pred_typhoon_with_viet_light, i=1, typhoon=typhoon, axis=1)

# Modify values
fiji_light['actual_perc_dmg'] = fiji_light['actual_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
fiji_light['pred_perc_dmg'] = fiji_light['pred_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
# Now calculate prediciton error
fiji_light['prediction_error'] = fiji_light['actual_perc_dmg'] - fiji_light['pred_perc_dmg'] # in percentual points

# Classic approach PHL+FJI+VIET + IWI AND light features
fiji_all = fiji.copy()
fiji_all['actual_perc_dmg'] = fiji_all.apply(calculate_actual_perc_dmg, df_fji=df_fji_complete_int,
                                            y_pred_typhoon=y_pred_typhoon_with_viet_all, i=1, typhoon=typhoon, axis=1)
fiji_all['pred_perc_dmg'] = fiji_all.apply(calculate_pred_perc_dmg, df_fji=df_fji_complete_int,
                                        y_pred_typhoon=y_pred_typhoon_with_viet_all, i=1, typhoon=typhoon, axis=1)

# Modify values
fiji_all['actual_perc_dmg'] = fiji_all['actual_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
fiji_all['pred_perc_dmg'] = fiji_all['pred_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
# Now calculate prediciton error
fiji_all['prediction_error'] = fiji_all['actual_perc_dmg'] - fiji_all['pred_perc_dmg'] # in percentual points

```


```python
# Load track
id = intersection[intersection['typhoon_name'] == typhoon].typhoon_id.to_list()
track = TCTracks.from_ibtracs_netcdf(storm_id=id)
tc_track = track.get_track()

points_ib = gpd.points_from_xy(tc_track.lon, tc_track.lat)
tc_track_line_ib = LineString(points_ib)

geometries_ib = gpd.GeoSeries([tc_track_line_ib])
line_gdf_ib = gpd.GeoDataFrame(geometry=geometries_ib)

# Plots
cmap='Reds'
cmap_blue = 'Blues'
cmap_red = 'Reds_r'
```

    2024-02-27 20:41:39,026 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.



```python
fig, ax = plt.subplots(2, 3, figsize=(15, 10))
ax = ax.flatten()
# Check prediction_error column for values > 0 and < 0
positive_error = fiji_iwi[fiji_iwi['prediction_error'] >= 0]
negative_error = fiji_iwi[fiji_iwi['prediction_error'] < 0]
positive_error_int = fiji_all[fiji_all['prediction_error'] >= 0]
negative_error_int = fiji_all[fiji_all['prediction_error'] < 0]

# Plotting the maps
fiji_plot_1 = fiji_iwi.plot(column='actual_perc_dmg', cmap=cmap, linewidth=0.2, ax=ax[0], edgecolor='0.3', legend=True)
fiji_plot_2 = fiji_iwi.plot(column='pred_perc_dmg', cmap=cmap, linewidth=0.2, ax=ax[1], edgecolor='0.3', legend=True)
fiji_plot_3_blue = positive_error.plot(column='prediction_error', cmap=cmap_blue, linewidth=0.2, ax=ax[2], edgecolor='0.3', vmin=0, legend=True)
fiji_plot_3_red = negative_error.plot(column='prediction_error', cmap=cmap_red, linewidth=0.2, ax=ax[2], edgecolor='0.3', vmax=0, legend=True)
fiji_plot_1_int = fiji_all.plot(column='actual_perc_dmg', cmap=cmap, linewidth=0.2, ax=ax[3], edgecolor='0.3', legend=True)
fiji_plot_2_int = fiji_all.plot(column='pred_perc_dmg', cmap=cmap, linewidth=0.2, ax=ax[4], edgecolor='0.3', legend=True)
fiji_plot_3_blue_int = positive_error_int.plot(column='prediction_error', cmap=cmap_blue, linewidth=0.2, ax=ax[5], edgecolor='0.3', vmin=0, legend=True)
fiji_plot_3_red_int = negative_error_int.plot(column='prediction_error', cmap=cmap_red, linewidth=0.2, ax=ax[5], edgecolor='0.3', vmax=0, legend=True)


line_gdf_ib.plot(ax=ax[0], color='k', linewidth=1, label='Typhoon track')  # Plot the LineString in black
line_gdf_ib.plot(ax=ax[1], color='k', linewidth=1, label='Typhoon track')  # Plot the LineString in black
line_gdf_ib.plot(ax=ax[2], color='k', linewidth=1, label='Typhoon track')  # Plot the LineString in black
line_gdf_ib.plot(ax=ax[3], color='k', linewidth=1, label='Typhoon track')  # Plot the LineString in black
line_gdf_ib.plot(ax=ax[4], color='k', linewidth=1, label='Typhoon track')  # Plot the LineString in black
line_gdf_ib.plot(ax=ax[5], color='k', linewidth=1, label='Typhoon track')  # Plot the LineString in black

# Create custom legends
blue_patch = mpatches.Patch(color='#6495ED', label='Underestimated damage')
red_patch = mpatches.Patch(color='#800000', label='Overestimated damage')
ax[2].legend(handles=[blue_patch, red_patch], loc='lower left', bbox_to_anchor=(-0.05, -0.2))
#ax[5].legend(handles=[blue_patch, red_patch], loc='lower left', bbox_to_anchor=(-0.05, -0.2))

ax[0].set_title('Actual Damage by municipality')
ax[1].set_title('Typhoon {} PHL+FJI+VIET model (With IWI feature) \n \nPredicted Damage by municipality'.format(typhoon))
ax[2].set_title('Prediction Error \n $actual_{dmg} - predicted_{dmg}$ \n(in percentage points)', y=0.95)
ax[4].set_title('Typhoon {} PHL+FJI+VIET model (With IWI + Light features)'.format(typhoon))
ax[0].axis('off')
ax[1].axis('off')
ax[2].axis('off')
ax[3].axis('off')
ax[4].axis('off')
ax[5].axis('off')

# All Fiji map
ax[0].set_xlim(176, 182)
ax[0].set_ylim(-20, -12)
ax[1].set_xlim(176, 182)
ax[1].set_ylim(-20, -12)
ax[2].set_xlim(176, 182)
ax[2].set_ylim(-20, -12)
ax[3].set_xlim(176, 182)
ax[3].set_ylim(-20, -12)
ax[4].set_xlim(176, 182)
ax[4].set_ylim(-20, -12)
ax[5].set_xlim(176, 182)
ax[5].set_ylim(-20, -12)

plt.tight_layout()
plt.show()
```



![png](09.1_combined_model_with_viet_count_using_pop_plus_features_files/09.1_combined_model_with_viet_count_using_pop_plus_features_26_0.png)




```python
actual_dmg = fiji_old['actual_perc_dmg']

pred_dmg_viet = fiji_old['pred_perc_dmg']
pred_dmg_viet_new = fiji_iwi['pred_perc_dmg']
pred_dmg_viet_new2 = fiji_light['pred_perc_dmg']
pred_dmg_viet_new3 = fiji_all['pred_perc_dmg']

error_baseline = fiji_old['actual_perc_dmg'] - fiji_old['actual_perc_dmg'] #basically 0
error_old = fiji_old['prediction_error']
error_new = fiji_iwi['prediction_error']
error_new2 = fiji_light['prediction_error']
error_new3 = fiji_all['prediction_error']
muns = fiji_old['NAME_2']
```


```python
plt.plot(error_baseline, 'o-',label='Perfect model', alpha=1)
plt.plot(error_old, 'o-', label='Classic features', alpha=0.4)
plt.plot(error_new, 'o-', label='+ IWI', alpha=0.4)
plt.plot(error_new2, 'o-', label='+ Light Index', alpha=0.4)
plt.plot(error_new3, 'o-', label='+ IWI and Light Index', alpha=0.4)

plt.xticks(range(len(muns)), muns, rotation='vertical')  # Set x-axis ticks and labels
plt.tight_layout()  # Adjust layout to prevent overlapping
plt.title('Tropical Cyclone: {}, PHL+FJI+VIET model'.format(typhoon))
plt.ylabel('% Points of damage \n difference')

plt.grid()
plt.legend(loc='upper left')
plt.show()
```



![png](09.1_combined_model_with_viet_count_using_pop_plus_features_files/09.1_combined_model_with_viet_count_using_pop_plus_features_28_0.png)



## Feature importance


```python
def model_importance(df_combined, df_fji, features, fji_weight, phl_weight=1, viet_weight=1):
    # Dataframe Fiji
    fji_typhoons = df_fji.typhoon_name.unique()

    # The model
    rmse_total_fji = []
    rmse_bin_fji = []
    avg_error_bin_fji = []

    y_test_typhoon_fji  = []
    y_pred_typhoon_fji  = []
    feature_importance_dict = {feature: [] for feature in features}
    shap_values_list = []
    for typhoon in fji_typhoons:

        """ PART 1: Train/Test """

        # LOOCV
        df_test = df_fji[df_fji["typhoon_name"] == typhoon] # Test set: Fiji
        #df_test = df_combined[df_combined["typhoon_name"] == typhoon] # Test set: Fiji
        df_train = df_combined[df_combined["typhoon_name"] != typhoon] # Train set: everything

        # Class weight
        weights = np.select(
            [
                (df_train['country'] == 'phl'),
                (df_train['country'] == 'viet'),
                (df_train['country'] == 'fji')
            ],
            [
                phl_weight,
                viet_weight,
                fji_weight
            ],
            default=1
        )

        # Split X and y from dataframe features
        X_test = df_test[features]
        X_train = df_train[features]

        y_train = df_train["percent_houses_damaged"]
        y_test = df_test["percent_houses_damaged"]

        # Stratify data
        bin_index_test = np.digitize(y_test, bins=bins[:-1])

        """ PART 2: XGB regressor """
        # create an XGBoost Regressor
        xgb = XGBRegressor(
            base_score=0.5,
            booster="gbtree",
            colsample_bylevel=0.8,
            colsample_bynode=0.8,
            colsample_bytree=0.8,
            gamma=3,
            eta=0.01,
            importance_type="gain",
            learning_rate=0.1,
            max_delta_step=0,
            max_depth=4,
            min_child_weight=1,
            missing=1,
            n_estimators=100,
            early_stopping_rounds=10,
            n_jobs=1,
            nthread=None,
            objective="reg:squarederror",
            reg_alpha=0,
            reg_lambda=1,
            scale_pos_weight=1,
            seed=None,
            silent=None,
            subsample=0.8,
            verbosity=0,
            eval_metric=["rmse", "logloss"],
            random_state=0,
        )

        # Fit the model
        eval_set = [(X_train, y_train)]
        xgb.fit(X_train, y_train, eval_set=eval_set, verbose=False, sample_weight=weights) #xgb_model

        # Get feature importance and append to dictionary
        feature_importance = xgb.feature_importances_
        for idx, feature in enumerate(features):
            feature_importance_dict[feature].append(feature_importance[idx])

        # Shap values
        # Initialize an explainer for the typhoon
        explainer = shap.Explainer(xgb, X_train)
        shap_values = explainer(X_test)
        shap_values_list.append(shap_values)

    return shap_values_list, feature_importance_dict
```


```python
shap_old, fi_old = model_importance(df_combined=df_combined_with_viet,
                                    df_fji=df_fji_with_viet,
                                    fji_weight=3,
                                    features=features_old)
```

    [15:20:28] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [15:20:35] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [15:20:39] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [15:20:46] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [15:20:53] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [15:21:00] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [15:21:06] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [15:21:12] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [15:21:16] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.



```python
shap_iwi, fi_iwi = model_importance(df_combined=df_combined_with_viet,
                                    df_fji=df_fji_with_viet,
                                    fji_weight=3,
                                    features=features_IWI)
```

    [16:00:31] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:00:39] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:00:46] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:00:53] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:01:00] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:01:06] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:01:12] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:01:18] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:01:24] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.



```python
shap_li, fi_li = model_importance(df_combined=df_combined_with_viet,
                                    df_fji=df_fji_with_viet,
                                    fji_weight=3,
                                    features=features_light)
```

    [16:36:57] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:37:04] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:37:11] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:37:19] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:37:26] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:37:33] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:37:38] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:37:45] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:37:51] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:37:52] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.



```python
shap_all, fi_all = model_importance(df_combined=df_combined_with_viet,
                                    df_fji=df_fji_with_viet,
                                    fji_weight=3,
                                    features=features_all)
```

    [16:01:37] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:01:44] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:01:51] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:01:59] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:02:06] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:02:10] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:02:14] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:02:21] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
    [16:02:28] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.



```python
fi = fi_old.copy()
shaps = shap_old.copy()
suptitle = 'No vulnerability features'

# Compute mean importance
mean_importance = {feature: np.mean(importance_list) for feature, importance_list in fi.items()}

# Compute mean SHAP values
#OBS: [shap_old[0].values[:,i].mean() for i in range(len(features))] is just basically shap_old[0].mean(0)
mean_shap_values = shaps[0].mean(0)
for shap_values in shaps[1:]:
    mean_shap_values += shap_values.mean(0)
mean_shap_values /= len(shaps)

fig, [ax, ax2] = plt.subplots(1,2, figsize=(10, 5))

# Sort feature importance in descending order
sorted_importance = {k: v for k, v in sorted(mean_importance.items(), key=lambda item: item[1], reverse=False)}
sorted_values = list(sorted_importance.values())
sorted_keys = list(sorted_importance.keys())
ax.barh(range(len(sorted_importance)), sorted_values, align="center", color='skyblue')
ax.set_yticks(range(len(sorted_importance)), sorted_keys)
ax.set_xlabel("Mean Importance")
ax.set_title("Feature Importance \n(XGB built-in feature)")


# Extract feature names and SHAP values from the explanation object
shap_features = mean_shap_values.feature_names
shap_features_values = mean_shap_values.values
# Sort features based on their absolute mean SHAP values
sorted_indices = np.argsort(np.abs(shap_features_values))#[::-1]
sorted_features_names = [shap_features[i] for i in sorted_indices]
sorted_shap_values_abs = np.abs(shap_features_values[sorted_indices])

ax2.barh(sorted_features_names, sorted_shap_values_abs, color='skyblue')
ax2.set_xlabel('Mean SHAP Value')
ax2.set_title('|SHAP Values|')

plt.suptitle(suptitle)
plt.tight_layout()
plt.show()
```



![png](09.1_combined_model_with_viet_count_using_pop_plus_features_files/09.1_combined_model_with_viet_count_using_pop_plus_features_35_0.png)




```python
fi = fi_old.copy()
shaps = shap_old.copy()
suptitle = 'No vulnerability features'

# Compute mean importance
mean_importance = {feature: np.mean(importance_list) for feature, importance_list in fi.items()}

# Compute mean SHAP values
#OBS: [shap_old[0].values[:,i].mean() for i in range(len(features))] is just basically shap_old[0].mean(0)
mean_shap_values = shaps[0].mean(0)
for shap_values in shaps[1:]:
    mean_shap_values += shap_values.mean(0)
mean_shap_values /= len(shaps)

fig, ax = plt.subplots(1,1, figsize=(5, 5))

# Sort feature importance in descending order
sorted_importance = {k: v for k, v in sorted(mean_importance.items(), key=lambda item: item[1], reverse=False)}
sorted_values = list(sorted_importance.values())
sorted_keys = list(sorted_importance.keys())
ax.barh(range(len(sorted_importance)), sorted_values, align="center", color='skyblue')
ax.set_yticks(range(len(sorted_importance)), sorted_keys)
ax.set_xlabel("Mean Importance")
ax.set_title("Feature Importance \n(XGB built-in feature)")

plt.suptitle(suptitle, y=1.01)
plt.show()
```



![png](09.1_combined_model_with_viet_count_using_pop_plus_features_files/09.1_combined_model_with_viet_count_using_pop_plus_features_36_0.png)




```python
fi = fi_li.copy()
shaps = shap_li.copy()
suptitle = '+ Light index vulnerability feature'

# Compute mean importance
mean_importance = {feature: np.mean(importance_list) for feature, importance_list in fi.items()}

# Compute mean SHAP values
#OBS: [shap_old[0].values[:,i].mean() for i in range(len(features))] is just basically shap_old[0].mean(0)
mean_shap_values = shaps[0].mean(0)
for shap_values in shaps[1:]:
    mean_shap_values += shap_values.mean(0)
mean_shap_values /= len(shaps)

fig, [ax, ax2] = plt.subplots(1,2, figsize=(10, 5))

# Sort feature importance in descending order
sorted_importance = {k: v for k, v in sorted(mean_importance.items(), key=lambda item: item[1], reverse=False)}
sorted_values = list(sorted_importance.values())
sorted_keys = list(sorted_importance.keys())
ax.barh(range(len(sorted_importance)), sorted_values, align="center", color='skyblue')
ax.set_yticks(range(len(sorted_importance)), sorted_keys)
ax.set_xlabel("Mean Importance")
ax.set_title("Feature Importance \n(XGB built-in feature)")


# Extract feature names and SHAP values from the explanation object
shap_features = mean_shap_values.feature_names
shap_features_values = mean_shap_values.values
# Sort features based on their absolute mean SHAP values
sorted_indices = np.argsort(np.abs(shap_features_values))#[::-1]
sorted_features_names = [shap_features[i] for i in sorted_indices]
sorted_shap_values_abs = np.abs(shap_features_values[sorted_indices])

ax2.barh(sorted_features_names, sorted_shap_values_abs, color='skyblue')
ax2.set_xlabel('Mean SHAP Value')
ax2.set_title('|SHAP Values|')

plt.suptitle(suptitle)
plt.tight_layout()
plt.show()
```



![png](09.1_combined_model_with_viet_count_using_pop_plus_features_files/09.1_combined_model_with_viet_count_using_pop_plus_features_37_0.png)




```python
fi = fi_li.copy()
shaps = shap_li.copy()
suptitle = '+ Light index vulnerability feature'

# Compute mean importance
mean_importance = {feature: np.mean(importance_list) for feature, importance_list in fi.items()}

# Compute mean SHAP values
#OBS: [shap_old[0].values[:,i].mean() for i in range(len(features))] is just basically shap_old[0].mean(0)
mean_shap_values = shaps[0].mean(0)
for shap_values in shaps[1:]:
    mean_shap_values += shap_values.mean(0)
mean_shap_values /= len(shaps)

fig, ax = plt.subplots(1,1, figsize=(5, 5))

# Sort feature importance in descending order
sorted_importance = {k: v for k, v in sorted(mean_importance.items(), key=lambda item: item[1], reverse=False)}
sorted_values = list(sorted_importance.values())
sorted_keys = list(sorted_importance.keys())
ax.barh(range(len(sorted_importance)), sorted_values, align="center", color='skyblue')
ax.set_yticks(range(len(sorted_importance)), sorted_keys)
ax.set_xlabel("Mean Importance")
ax.set_title("Feature Importance \n(XGB built-in feature)")

plt.suptitle(suptitle, y=1.01)
plt.show()
```



![png](09.1_combined_model_with_viet_count_using_pop_plus_features_files/09.1_combined_model_with_viet_count_using_pop_plus_features_38_0.png)




```python
fi = fi_iwi.copy()
shaps = shap_iwi.copy()
suptitle = '+ IWI vulnerability feature'

# Compute mean importance
mean_importance = {feature: np.mean(importance_list) for feature, importance_list in fi.items()}

# Compute mean SHAP values
#OBS: [shap_old[0].values[:,i].mean() for i in range(len(features))] is just basically shap_old[0].mean(0)
mean_shap_values = shaps[0].mean(0)
for shap_values in shaps[1:]:
    mean_shap_values += shap_values.mean(0)
mean_shap_values /= len(shaps)

fig, [ax, ax2] = plt.subplots(1,2, figsize=(10, 5))

# Sort feature importance in descending order
sorted_importance = {k: v for k, v in sorted(mean_importance.items(), key=lambda item: item[1], reverse=False)}
sorted_values = list(sorted_importance.values())
sorted_keys = list(sorted_importance.keys())
ax.barh(range(len(sorted_importance)), sorted_values, align="center", color='skyblue')
ax.set_yticks(range(len(sorted_importance)), sorted_keys)
ax.set_xlabel("Mean Importance")
ax.set_title("Feature Importance \n(XGB built-in feature)")


# Extract feature names and SHAP values from the explanation object
shap_features = mean_shap_values.feature_names
shap_features_values = mean_shap_values.values
# Sort features based on their absolute mean SHAP values
sorted_indices = np.argsort(np.abs(shap_features_values))#[::-1]
sorted_features_names = [shap_features[i] for i in sorted_indices]
sorted_shap_values_abs = np.abs(shap_features_values[sorted_indices])

ax2.barh(sorted_features_names, sorted_shap_values_abs, color='skyblue')
ax2.set_xlabel('Mean SHAP Value')
ax2.set_title('|SHAP Values|')

plt.suptitle(suptitle)
plt.tight_layout()
plt.show()
```



![png](09.1_combined_model_with_viet_count_using_pop_plus_features_files/09.1_combined_model_with_viet_count_using_pop_plus_features_39_0.png)




```python
fi = fi_iwi.copy()
shaps = shap_iwi.copy()
suptitle = '+ IWI vulnerability feature'

# Compute mean importance
mean_importance = {feature: np.mean(importance_list) for feature, importance_list in fi.items()}

# Compute mean SHAP values
#OBS: [shap_old[0].values[:,i].mean() for i in range(len(features))] is just basically shap_old[0].mean(0)
mean_shap_values = shaps[0].mean(0)
for shap_values in shaps[1:]:
    mean_shap_values += shap_values.mean(0)
mean_shap_values /= len(shaps)

fig, ax = plt.subplots(1,1, figsize=(5, 5))

# Sort feature importance in descending order
sorted_importance = {k: v for k, v in sorted(mean_importance.items(), key=lambda item: item[1], reverse=False)}
sorted_values = list(sorted_importance.values())
sorted_keys = list(sorted_importance.keys())
ax.barh(range(len(sorted_importance)), sorted_values, align="center", color='skyblue')
ax.set_yticks(range(len(sorted_importance)), sorted_keys)
ax.set_xlabel("Mean Importance")
ax.set_title("Feature Importance \n(XGB built-in feature)")

plt.suptitle(suptitle, y=1.01)
plt.show()
```



![png](09.1_combined_model_with_viet_count_using_pop_plus_features_files/09.1_combined_model_with_viet_count_using_pop_plus_features_40_0.png)




```python
fi = fi_all.copy()
shaps = shap_all.copy()
suptitle = 'All vulnerability features'

# Compute mean importance
mean_importance = {feature: np.mean(importance_list) for feature, importance_list in fi.items()}

# Compute mean SHAP values
#OBS: [shap_old[0].values[:,i].mean() for i in range(len(features))] is just basically shap_old[0].mean(0)
mean_shap_values = shaps[0].mean(0)
for shap_values in shaps[1:]:
    mean_shap_values += shap_values.mean(0)
mean_shap_values /= len(shaps)

fig, [ax, ax2] = plt.subplots(1,2, figsize=(10, 5))

# Sort feature importance in descending order
sorted_importance = {k: v for k, v in sorted(mean_importance.items(), key=lambda item: item[1], reverse=False)}
sorted_values = list(sorted_importance.values())
sorted_keys = list(sorted_importance.keys())
ax.barh(range(len(sorted_importance)), sorted_values, align="center", color='skyblue')
ax.set_yticks(range(len(sorted_importance)), sorted_keys)
ax.set_xlabel("Mean Importance")
ax.set_title("Feature Importance \n(XGB built-in feature)")


# Extract feature names and SHAP values from the explanation object
shap_features = mean_shap_values.feature_names
shap_features_values = mean_shap_values.values
# Sort features based on their absolute mean SHAP values
sorted_indices = np.argsort(np.abs(shap_features_values))#[::-1]
sorted_features_names = [shap_features[i] for i in sorted_indices]
sorted_shap_values_abs = np.abs(shap_features_values[sorted_indices])

ax2.barh(sorted_features_names, sorted_shap_values_abs, color='skyblue')
ax2.set_xlabel('Mean SHAP Value')
ax2.set_title('|SHAP Values|')

plt.suptitle(suptitle)
plt.tight_layout()
plt.show()
```



![png](09.1_combined_model_with_viet_count_using_pop_plus_features_files/09.1_combined_model_with_viet_count_using_pop_plus_features_41_0.png)




```python
fi = fi_all.copy()
shaps = shap_all.copy()
suptitle = 'All vulnerability features'

# Compute mean importance
mean_importance = {feature: np.mean(importance_list) for feature, importance_list in fi.items()}

# Compute mean SHAP values
#OBS: [shap_old[0].values[:,i].mean() for i in range(len(features))] is just basically shap_old[0].mean(0)
mean_shap_values = shaps[0].mean(0)
for shap_values in shaps[1:]:
    mean_shap_values += shap_values.mean(0)
mean_shap_values /= len(shaps)

fig, ax = plt.subplots(1,1, figsize=(5, 5))

# Sort feature importance in descending order
sorted_importance = {k: v for k, v in sorted(mean_importance.items(), key=lambda item: item[1], reverse=False)}
sorted_values = list(sorted_importance.values())
sorted_keys = list(sorted_importance.keys())
ax.barh(range(len(sorted_importance)), sorted_values, align="center", color='skyblue')
ax.set_yticks(range(len(sorted_importance)), sorted_keys)
ax.set_xlabel("Mean Importance")
ax.set_title("Feature Importance \n(XGB built-in feature)")

plt.suptitle(suptitle, y=1.01)
plt.show()
```



![png](09.1_combined_model_with_viet_count_using_pop_plus_features_files/09.1_combined_model_with_viet_count_using_pop_plus_features_42_0.png)
