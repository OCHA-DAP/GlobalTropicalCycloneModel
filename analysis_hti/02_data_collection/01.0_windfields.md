```python
from pathlib import Path
import os

from climada.hazard import Centroids, TCTracks, TropCyclone
from shapely.geometry import LineString
import geopandas as gpd
import numpy as np
import pandas as pd
import xarray as xr
import matplotlib.pyplot as plt

import warnings

# Ignore specific UserWarnings globally
warnings.filterwarnings("ignore", message="Geometry is in a geographic CRS.*")
warnings.filterwarnings("ignore", message="Use 'GeoSeries.to_crs()'.*")
```


```python
DEG_TO_KM = 111.1  # Convert 1 degree to km
input_dir = Path(os.getenv("STORM_DATA_DIR")) / "analysis_hti/02_model_features"
output_dir = input_dir / "01_windfield"
# Create the directory if it doesn't exist
os.makedirs(output_dir, exist_ok=True)
```

## Get typhoon data


```python
# House impact data
housing_path_in = input_dir / '02_housing_damage/input'
df_housing = pd.read_csv(housing_path_in / 'impact_data_clean_hti.csv')

# List of typhoons
typhoons_df = df_housing[['typhoon_name', 'Year', 'sid']].drop_duplicates().reset_index(drop=True)
intersection = typhoons_df.copy()
```


```python
# Download NECESARY tracks
sel_ibtracs = []
for track in intersection.sid:
    sel_ibtracs.append(TCTracks.from_ibtracs_netcdf(storm_id=track))
```

    2024-04-02 15:17:17,017 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:17:20,000 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:17:23,089 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:17:26,178 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:17:29,195 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:17:32,264 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:17:35,277 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:17:38,285 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:17:41,367 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:17:44,455 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:17:47,515 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:17:50,563 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:17:53,565 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:17:56,546 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:17:59,604 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:18:02,629 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:18:05,725 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:18:08,835 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:18:11,937 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:18:15,190 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:18:18,299 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:18:21,399 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:18:24,566 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:18:27,838 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.
    2024-04-02 15:18:31,120 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.



```python
# Interpolation
#obs: .interp(x0,x,f(x)) gives the position of x0 in the fitting of (x,f(x))
#obs: daterange consider the track between certain intervals as discrete points instead of a continuous
tc_tracks = TCTracks()
for track in sel_ibtracs:
    tc_track = track.get_track()
    tc_track.interp(
        time = pd.date_range(tc_track.time.values[0], tc_track.time.values[-1], freq="30T")
    )
    tc_tracks.append(tc_track)
```


```python
# Plot the tracks
# Takes a while, especially after the interpolation.
ax = tc_tracks.plot()
ax.set_title('Haiti and surroundings Typhoon Tracks', size=20)
plt.show()
```



![png](01.0_windfields_files/01.0_windfields_6_0.png)



## Define some functions


```python
def windfield_to_grid(tc, tracks, grids):
    df_windfield = pd.DataFrame()

    for intensity_sparse, event_id in zip(tc.intensity, tc.event_name):
        # Get the windfield
        windfield = intensity_sparse.toarray().flatten()
        npoints = len(windfield)
        # Get the track distance
        tc_track = tracks.get_track(track_name=event_id)
        points = gpd.points_from_xy(tc_track.lon, tc_track.lat)
        tc_track_line = LineString(points)
        DEG_TO_KM = 111.1
        tc_track_distance = grids["geometry"].apply(
            lambda point: point.distance(tc_track_line) * DEG_TO_KM
        )
        # Add to DF
        df_to_add = pd.DataFrame(
            dict(
                typhoon_name=[tc_track.name] * npoints,
                typhoon_year=[int(tc_track.sid[:4])] * npoints,
                track_id=[event_id] * npoints,
                grid_point_id=grids["id"],
                wind_speed=windfield,
                track_distance=tc_track_distance,
                geometry = grids.geometry
            )
        )
        df_windfield = pd.concat([df_windfield, df_to_add], ignore_index=True)
    return df_windfield

# Define a function to calculate mean values for neighboring cells
def calculate_mean_for_neighbors(idx, gdf, buffer_size):
    row = gdf.iloc[idx]
    if row['wind_speed'] == 0:  # Check if wind_speed is 0
        buffered = row['geometry'].buffer(buffer_size)  # Adjust buffer size as needed

        # Find neighboring geometries that intersect with the buffer, excluding the current geometry
        neighbors = gdf[~gdf.geometry.equals(row['geometry']) & gdf.geometry.intersects(buffered)]

        if not neighbors.empty:
            # drop rows with 0 windspeed vals (we dont want to compute the mean while considering these cells)
            neighbors = neighbors[neighbors['wind_speed'] !=0]
            if len(neighbors) !=0:
                mean_val = neighbors['wind_speed'].mean()
            else:
                mean_val = 0
            return mean_val
    return row['wind_speed']  # Return the original value if no neighbors or wind_speed != 0

# Function to add interpolation points
def add_interpolation_points(data, num_points_between):
    new_x_list = []
    for i in range(len(data) - 1):
        start_point, end_point = data[i], data[i + 1]
        interp_x = list(np.linspace(start_point, end_point, num_points_between + 2))
        if i == 0:
            new_x_list.append(interp_x)
        elif i == (len(data) - 1):
            new_x_list.append(interp_x)
        else:
            new_x_list.append(interp_x[1:])

    new_x = np.concatenate(new_x_list)

    return new_x

# Create xarray
def adjust_tracks(forecast_df):
    track = xr.Dataset(
        data_vars={
            'max_sustained_wind': ('time', np.array(forecast_df.MeanWind.values, dtype='float32')), #0.514444 --> kn to m/s
            'environmental_pressure': ('time', forecast_df.PressureOCI.values), # I assume its enviromental pressure
            'central_pressure': ('time',forecast_df.Pressure.values),
            'lat': ('time',forecast_df.Latitude.values),
            'lon': ('time', forecast_df.Longitude.values),
            'radius_max_wind': ('time', forecast_df.RadiusMaxWinds.values),
            'radius_oci': ('time',forecast_df.RadiusOCI.values), # Works even if there is a bunch of nans. Doesnt change the windspeed values
            'time_step': ('time', forecast_df.time_step),
            'basin': ('time', np.array(forecast_df.basin, dtype='<U2'))
        },
        coords={
            'time': forecast_df.forecast_time.values,
        },
        attrs={
            'max_sustained_wind_unit': 'kn',
            'central_pressure_unit': 'mb',
            'name': name,
            'sid' : custom_sid,
            'orig_event_flag': True,
            'data_provider': 'Custom',
            'id_no' : custom_idno,
            'category': int(max(forecast_df.Category.iloc)),
        }
    )
    track = track.set_coords(['lat', 'lon'])
    return track
```

## Construct the windfield


```python
# Just grid-land overlap
filepath = (
    input_dir
    / "02_housing_damage/output/hti_0.1_degree_grid_centroids_land_overlap.gpkg"
)
gdf = gpd.read_file(filepath)
# Include oceans
filepath_complete = (
    input_dir
    / "02_housing_damage/output/hti_0.1_degree_grid_centroids.gpkg"
)
gdf_all = gpd.read_file(filepath_complete)

# Centroids
cent = Centroids.from_geodataframe(gdf) # grid-land overlap
cent_all = Centroids.from_geodataframe(gdf_all) # include oceans
```

    2024-04-03 13:03:39,029 - climada.hazard.centroids.centr - WARNING - Centroids.from_geodataframe has been deprecated and will be removed in a future version. Use ther default constructor instead.
    2024-04-03 13:03:39,042 - climada.hazard.centroids.centr - WARNING - Centroids.from_geodataframe has been deprecated and will be removed in a future version. Use ther default constructor instead.



```python
cent.check()
cent.plot()
plt.show()
```



![png](01.0_windfields_files/01.0_windfields_11_0.png)



Add interpolation points


```python
tracks = TCTracks()
for i in range(len(tc_tracks.get_track())):
    # Define relevant features
    track_xarray = tc_tracks.get_track()[i]
    time_array = np.array(track_xarray.time)
    time_step_array = np.array(track_xarray.time_step)
    lat_array = np.array(track_xarray.lat)
    lon_array = np.array(track_xarray.lon)
    max_sustained_wind_array = np.array(track_xarray.max_sustained_wind)
    central_pressure_array = np.array(track_xarray.central_pressure)
    environmental_pressure_array = np.array(track_xarray.environmental_pressure)
    r_max_wind_array = np.array(track_xarray.radius_max_wind)
    r_oci_array = np.array(track_xarray.radius_oci)

    # Define new variables
    # Interpolate every important data
    w = max_sustained_wind_array.copy()
    t = time_array.copy()
    t_step = time_step_array.copy()
    lat = lat_array.copy()
    lon = lon_array.copy()
    cp = central_pressure_array.copy()
    ep = environmental_pressure_array.copy()
    rmax = r_max_wind_array.copy()
    roci = r_oci_array.copy()

    # Define the number of points to add between each pair of data points
    num_points_between = 2

    # Add interpolation points to regulat variables
    new_w = add_interpolation_points(w, num_points_between)
    new_t_step = add_interpolation_points(t_step, num_points_between)
    new_lat = add_interpolation_points(lat, num_points_between)
    new_lon = add_interpolation_points(lon, num_points_between)
    new_cp = add_interpolation_points(cp, num_points_between)
    new_ep = add_interpolation_points(ep, num_points_between)
    new_rmax = add_interpolation_points(rmax, num_points_between)
    new_roci = add_interpolation_points(roci, num_points_between)

    # Add interpolation points to time variables
    timestamps = np.array([date.astype('datetime64[s]').astype('int64') for date in t])# Convert to seconds
    new_t =  add_interpolation_points(timestamps, num_points_between)
    new_t = [np.datetime64(int(ts), 's') for ts in new_t]# Back to datetime format

    # Define dataframe
    df_t = pd.DataFrame({
        'MeanWind': new_w,
        'PressureOCI': new_ep,
        'Pressure': new_cp,
        'Latitude': new_lat,
        'Longitude': new_lon,
        'RadiusMaxWinds': new_rmax,
        'RadiusOCI': new_roci,
        'time_step': new_t_step,
        'basin': np.array([np.array(track_xarray.basin)[0]] * len(new_t)),
        'forecast_time': new_t,
        'Category': track_xarray.category
    })

    # Define a custom id
    custom_idno = track_xarray.id_no
    custom_sid = track_xarray.sid
    name = track_xarray.name# + ' interpolated'

    # Define track as climada likes it
    track = TCTracks()
    track.data = [adjust_tracks(df_t)]

    # Tracks modified
    tracks.append(track.get_track())
```

Create TropCyclone class


```python
tc_all = TropCyclone.from_tracks(
    tracks, centroids=cent_all, store_windfields=True, intensity_thres=0
)

# Create grid-level windfield
df_windfield_interpolated = windfield_to_grid(tc=tc_all, tracks=tracks, grids=gdf_all)
```


```python
# Overlap
df_windfield_interpolated_overlap = df_windfield_interpolated[df_windfield_interpolated.grid_point_id.isin(gdf.id)]
```

## Sanity checks


```python
# Check nans
df_windfield_interpolated.isna().sum()
```




    typhoon_name      0
    track_id          0
    grid_point_id     0
    wind_speed        0
    track_distance    0
    geometry          0
    dtype: int64




```python
df_windfield_interpolated_overlap[df_windfield_interpolated_overlap.typhoon_name=='EMILY']
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>typhoon_name</th>
      <th>typhoon_year</th>
      <th>track_id</th>
      <th>grid_point_id</th>
      <th>wind_speed</th>
      <th>track_distance</th>
      <th>geometry</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7122</th>
      <td>EMILY</td>
      <td>2005</td>
      <td>2005192N11318</td>
      <td>235</td>
      <td>0.000000</td>
      <td>362.060649</td>
      <td>POINT (-74.45000 18.65000)</td>
    </tr>
    <tr>
      <th>7123</th>
      <td>EMILY</td>
      <td>2005</td>
      <td>2005192N11318</td>
      <td>236</td>
      <td>0.000000</td>
      <td>351.508725</td>
      <td>POINT (-74.45000 18.55000)</td>
    </tr>
    <tr>
      <th>7124</th>
      <td>EMILY</td>
      <td>2005</td>
      <td>2005192N11318</td>
      <td>237</td>
      <td>0.000000</td>
      <td>340.956802</td>
      <td>POINT (-74.45000 18.45000)</td>
    </tr>
    <tr>
      <th>7125</th>
      <td>EMILY</td>
      <td>2005</td>
      <td>2005192N11318</td>
      <td>238</td>
      <td>0.000000</td>
      <td>330.404878</td>
      <td>POINT (-74.45000 18.35000)</td>
    </tr>
    <tr>
      <th>7164</th>
      <td>EMILY</td>
      <td>2005</td>
      <td>2005192N11318</td>
      <td>277</td>
      <td>0.000000</td>
      <td>365.537574</td>
      <td>POINT (-74.35000 18.65000)</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>28955</th>
      <td>EMILY</td>
      <td>2011</td>
      <td>2011214N15299</td>
      <td>1404</td>
      <td>6.923630</td>
      <td>232.630867</td>
      <td>POINT (-71.65000 19.35000)</td>
    </tr>
    <tr>
      <th>28956</th>
      <td>EMILY</td>
      <td>2011</td>
      <td>2011214N15299</td>
      <td>1405</td>
      <td>7.124027</td>
      <td>222.516102</td>
      <td>POINT (-71.65000 19.25000)</td>
    </tr>
    <tr>
      <th>28957</th>
      <td>EMILY</td>
      <td>2011</td>
      <td>2011214N15299</td>
      <td>1406</td>
      <td>7.391463</td>
      <td>212.401337</td>
      <td>POINT (-71.65000 19.15000)</td>
    </tr>
    <tr>
      <th>28958</th>
      <td>EMILY</td>
      <td>2011</td>
      <td>2011214N15299</td>
      <td>1407</td>
      <td>7.668760</td>
      <td>202.286572</td>
      <td>POINT (-71.65000 19.05000)</td>
    </tr>
    <tr>
      <th>28965</th>
      <td>EMILY</td>
      <td>2011</td>
      <td>2011214N15299</td>
      <td>1414</td>
      <td>9.867125</td>
      <td>131.656056</td>
      <td>POINT (-71.65000 18.35000)</td>
    </tr>
  </tbody>
</table>
<p>642 rows Ã— 7 columns</p>
</div>



## Example


```python
grid_input = (
    Path(os.getenv("STORM_DATA_DIR"))
    / "analysis_hti/02_model_features/02_housing_damage/output/"
)
shape_input = (
    Path(os.getenv("STORM_DATA_DIR"))
    / "analysis_hti/02_model_features/02_housing_damage/input/"
)

# Load grid
grid_land_overlap = gpd.read_file(grid_input / "hti_0.1_degree_grid_land_overlap.gpkg")
grid_land_overlap["id"] = grid_land_overlap["id"].astype(int)
grid = grid_land_overlap.copy()

# Load shapefile
shp = gpd.read_file(
    shape_input / "shapefile_hti_fixed.gpkg"
)
shp = shp.to_crs('EPSG:4326')
```


```python
# Track path
tc_track = tracks.get_track()[2]
points = gpd.points_from_xy(tc_track.lon, tc_track.lat)
track_points = gpd.GeoDataFrame(geometry=points)
tc_track_line = LineString(points)
track_line = gpd.GeoDataFrame(geometry=[tc_track_line])

fig, ax = plt.subplots(1,1, figsize=(5,5))
# Plot intensity
name = tc_track.name
geo_windfield = gpd.GeoDataFrame(df_windfield_interpolated_overlap[df_windfield_interpolated_overlap.typhoon_name == name])
geo_grid_wind = grid.merge(geo_windfield[['grid_point_id', 'wind_speed']], left_on='id', right_on='grid_point_id')

shp.plot(color='gray', ax=ax)
geo_grid_wind.plot(column='wind_speed', cmap='Reds', linewidth=0.2, edgecolor='0.3', ax=ax, legend=True)
track_line.plot(ax=ax, color='k', linewidth=1, label='Typhoon track')

#ax.axis('off')
ax.set_xlim([-76, -70])
ax.set_ylim([17, 21])
ax.set_title('{}\nWindspeed [m/s]'.format(name))
plt.show()
```



![png](01.0_windfields_files/01.0_windfields_22_0.png)



## Save everything


```python
df_windfield_interpolated_overlap.to_csv(output_dir / "windfield_data_hti_overlap.csv", index = False)
```

## Build metadata


```python
from datetime import datetime
# Load shp
shp_path = input_dir / '02_housing_damage/input/'
shp = gpd.read_file(
    shp_path / "shapefile_hti_fixed.gpkg"
)
shp = shp.to_crs('EPSG:4326')
```


```python
df_metadata = pd.DataFrame()
for i in range(len(tracks.data)):
    # Basics
    startdate = np.datetime64(np.array(tracks.data[i].time[0]), 'D')
    enddate = np.datetime64(np.array(tracks.data[i].time[-1]), 'D')
    name = tracks.data[i].name
    year = tracks.data[i].sid[:4]
    nameyear = name + year

    # For the landfall
    # Track path
    tc_track = tracks.get_track()[i]
    points = gpd.points_from_xy(tc_track.lon, tc_track.lat)
    track_points = gpd.GeoDataFrame(geometry=points)

    # Set crs
    track_points.crs = shp.crs

    try:
        # intersection --> Look for first intersection == landfall
        min_index = shp.sjoin(track_points)['index_right'].min()

        landfalldate = np.datetime64(np.array(tracks.data[i].time[min_index]), 'D')
        landfall_time = str(np.datetime64(np.array(tracks.data[i].time[min_index]), 's')).split('T')[1]
    except:
        # No landfall situation
        landfalldate = np.nan
        landfall_time = np.nan

    # Create df
    df_aux = pd.DataFrame({
        'typhoon': [nameyear],
        'startdate': [startdate],
        'enddate': [enddate],
        'landfalldate': [landfalldate],
        'landfall_time': [landfall_time]
    }
    )
    df_metadata = pd.concat([df_metadata, df_aux])
df_metadata = df_metadata.reset_index(drop=True)
```

    /var/folders/dy/vms3cfrn4q9952h8s6l586dr0000gp/T/ipykernel_4978/3368193592.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
      df_metadata = pd.concat([df_metadata, df_aux])
    /var/folders/dy/vms3cfrn4q9952h8s6l586dr0000gp/T/ipykernel_4978/3368193592.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
      df_metadata = pd.concat([df_metadata, df_aux])
    /var/folders/dy/vms3cfrn4q9952h8s6l586dr0000gp/T/ipykernel_4978/3368193592.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
      df_metadata = pd.concat([df_metadata, df_aux])
    /var/folders/dy/vms3cfrn4q9952h8s6l586dr0000gp/T/ipykernel_4978/3368193592.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
      df_metadata = pd.concat([df_metadata, df_aux])
    /var/folders/dy/vms3cfrn4q9952h8s6l586dr0000gp/T/ipykernel_4978/3368193592.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
      df_metadata = pd.concat([df_metadata, df_aux])
    /var/folders/dy/vms3cfrn4q9952h8s6l586dr0000gp/T/ipykernel_4978/3368193592.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
      df_metadata = pd.concat([df_metadata, df_aux])
    /var/folders/dy/vms3cfrn4q9952h8s6l586dr0000gp/T/ipykernel_4978/3368193592.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
      df_metadata = pd.concat([df_metadata, df_aux])
    /var/folders/dy/vms3cfrn4q9952h8s6l586dr0000gp/T/ipykernel_4978/3368193592.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
      df_metadata = pd.concat([df_metadata, df_aux])
    /var/folders/dy/vms3cfrn4q9952h8s6l586dr0000gp/T/ipykernel_4978/3368193592.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
      df_metadata = pd.concat([df_metadata, df_aux])
    /var/folders/dy/vms3cfrn4q9952h8s6l586dr0000gp/T/ipykernel_4978/3368193592.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
      df_metadata = pd.concat([df_metadata, df_aux])
    /var/folders/dy/vms3cfrn4q9952h8s6l586dr0000gp/T/ipykernel_4978/3368193592.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
      df_metadata = pd.concat([df_metadata, df_aux])



```python
df_metadata.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>typhoon</th>
      <th>startdate</th>
      <th>enddate</th>
      <th>landfalldate</th>
      <th>landfall_time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>LILI2002</td>
      <td>2002-09-21</td>
      <td>2002-10-04</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>IVAN2004</td>
      <td>2004-09-02</td>
      <td>2004-09-24</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>JEANNE2004</td>
      <td>2004-09-13</td>
      <td>2004-09-29</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>DENNIS2005</td>
      <td>2005-07-04</td>
      <td>2005-07-18</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>EMILY2005</td>
      <td>2005-07-11</td>
      <td>2005-07-21</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>5</th>
      <td>STAN2005</td>
      <td>2005-10-01</td>
      <td>2005-10-05</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>6</th>
      <td>ALPHA2005</td>
      <td>2005-10-22</td>
      <td>2005-10-24</td>
      <td>2005-10-23</td>
      <td>11:20:00</td>
    </tr>
    <tr>
      <th>7</th>
      <td>ERNESTO2006</td>
      <td>2006-08-24</td>
      <td>2006-09-04</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>8</th>
      <td>DEAN2007</td>
      <td>2007-08-13</td>
      <td>2007-08-23</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>9</th>
      <td>NOEL2007</td>
      <td>2007-10-24</td>
      <td>2007-11-06</td>
      <td>2007-10-29</td>
      <td>08:20:00</td>
    </tr>
  </tbody>
</table>
</div>



For some events there are not landafall info


```python
nans_meta = df_metadata[df_metadata.landfall_time.isna()]
nans_meta
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>typhoon</th>
      <th>startdate</th>
      <th>enddate</th>
      <th>landfalldate</th>
      <th>landfall_time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>LILI2002</td>
      <td>2002-09-21</td>
      <td>2002-10-04</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>IVAN2004</td>
      <td>2004-09-02</td>
      <td>2004-09-24</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>JEANNE2004</td>
      <td>2004-09-13</td>
      <td>2004-09-29</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>DENNIS2005</td>
      <td>2005-07-04</td>
      <td>2005-07-18</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>EMILY2005</td>
      <td>2005-07-11</td>
      <td>2005-07-21</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>5</th>
      <td>STAN2005</td>
      <td>2005-10-01</td>
      <td>2005-10-05</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>7</th>
      <td>ERNESTO2006</td>
      <td>2006-08-24</td>
      <td>2006-09-04</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>8</th>
      <td>DEAN2007</td>
      <td>2007-08-13</td>
      <td>2007-08-23</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>13</th>
      <td>HANNA2008</td>
      <td>2008-08-28</td>
      <td>2008-09-08</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>14</th>
      <td>IKE2008</td>
      <td>2008-09-01</td>
      <td>2008-09-15</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>15</th>
      <td>TOMAS2010</td>
      <td>2010-10-29</td>
      <td>2010-11-10</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>17</th>
      <td>IRENE2011</td>
      <td>2011-08-21</td>
      <td>2011-08-30</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>19</th>
      <td>SANDY2012</td>
      <td>2012-10-21</td>
      <td>2012-10-31</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>20</th>
      <td>ERIKA2015</td>
      <td>2015-08-24</td>
      <td>2015-08-28</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>22</th>
      <td>IRMA2017</td>
      <td>2017-08-30</td>
      <td>2017-09-13</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>24</th>
      <td>ELSA2021</td>
      <td>2021-06-30</td>
      <td>2021-07-10</td>
      <td>NaT</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>




```python
fig, ax = plt.subplots(1,5, figsize = (15,8))
for i in range(5):
    j = nans_meta.index[i]
    tc_track = tracks.get_track()[j]
    points = gpd.points_from_xy(tc_track.lon, tc_track.lat)
    track_points = gpd.GeoDataFrame(geometry=points)
    #Plot
    shp.plot(ax=ax[i])
    track_points.plot(ax=ax[i], color='k', label='track')
    track_points.iloc[0:2].plot(ax=ax[i], color = 'r', label='beginning')

    ax[i].legend()
    ax[i].set_title(nans_meta.iloc[i].typhoon)
    ax[i].set_xlim([-76, -70])
    ax[i].set_ylim([16, 22])

plt.show()
```



![png](01.0_windfields_files/01.0_windfields_31_0.png)



Solve the problem


```python
from shapely.geometry import Point

def get_closest_point_index(track_points, shp):
    """
    Find the index of the closest point in track_points to the shapefile shp.

    Parameters:
    track_points (GeoDataFrame): GeoPandas DataFrame representing track points.
    shp (GeoDataFrame): GeoPandas DataFrame representing a shapefile.

    Returns:
    int: Index of the closest point in track_points.
    """
    # Convert shapefile to a single polygon (if it contains multiple geometries)
    shp_polygon = shp.unary_union

    # Calculate distance to each point in track_points
    closest_distance = float('inf')  # Initialize with infinity
    closest_point_index = None

    for i, point in enumerate(track_points.geometry):
        # Calculate distance to the shapefile
        distance = point.distance(shp_polygon)

        # Update closest point if this point is closer
        if distance < closest_distance:
            closest_distance = distance
            closest_point_index = i

    return closest_point_index

```


```python
fig, ax = plt.subplots(1,5, figsize = (15,8))
for i in range(5):
    j = nans_meta.index[i]
    tc_track = tracks.get_track()[j]
    points = gpd.points_from_xy(tc_track.lon, tc_track.lat)
    track_points = gpd.GeoDataFrame(geometry=points)
    closest_point = get_closest_point_index(track_points, shp)

    points2 = points[closest_point]
    track_points2 = gpd.GeoDataFrame(geometry=[points2])

    #Plot
    shp.plot(ax=ax[i])
    track_points.plot(ax=ax[i], color='k', label='track')
    track_points2.plot(ax=ax[i], color='r', alpha=0.6, label='Closest point')


    ax[i].legend()
    ax[i].set_title(nans_meta.iloc[i].typhoon)
    ax[i].set_xlim([-76, -70])
    ax[i].set_ylim([16, 22])

plt.show()
```



![png](01.0_windfields_files/01.0_windfields_34_0.png)




```python
df_metadata_fixed = pd.DataFrame()
for i in range(len(tracks.data)):
    # Basics
    startdate = np.datetime64(np.array(tracks.data[i].time[0]), 'D')
    enddate = np.datetime64(np.array(tracks.data[i].time[-1]), 'D')
    name = tracks.data[i].name
    year = tracks.data[i].sid[:4]
    nameyear = name + year

    # For the landfall
    # Track path
    tc_track = tracks.get_track()[i]
    points = gpd.points_from_xy(tc_track.lon, tc_track.lat)
    track_points = gpd.GeoDataFrame(geometry=points)

    # Set crs
    track_points.crs = shp.crs

    try:
        # intersection --> Look for first intersection == landfall
        min_index = shp.sjoin(track_points)['index_right'].min()

        landfalldate = np.datetime64(np.array(tracks.data[i].time[min_index]), 'D')
        landfall_time = str(np.datetime64(np.array(tracks.data[i].time[min_index]), 's')).split('T')[1]
    except:
        # No landfall situation --> Use closest point to shapefile
        closest_point_index = get_closest_point_index(track_points, shp)
        landfalldate = np.datetime64(np.array(tracks.data[i].time[closest_point_index]), 'D')
        landfall_time = str(np.datetime64(np.array(tracks.data[i].time[closest_point_index]), 's')).split('T')[1]

    # Create df
    df_aux = pd.DataFrame({
        'typhoon': [nameyear],
        'startdate': [startdate],
        'enddate': [enddate],
        'landfalldate': [landfalldate],
        'landfall_time': [landfall_time]
    }
    )
    df_metadata_fixed = pd.concat([df_metadata_fixed, df_aux])
df_metadata_fixed = df_metadata_fixed.reset_index(drop=True)
```


```python
df_metadata_fixed
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>typhoon</th>
      <th>startdate</th>
      <th>enddate</th>
      <th>landfalldate</th>
      <th>landfall_time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>LILI2002</td>
      <td>2002-09-21</td>
      <td>2002-10-04</td>
      <td>2002-09-28</td>
      <td>12:00:00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>IVAN2004</td>
      <td>2004-09-02</td>
      <td>2004-09-24</td>
      <td>2004-09-10</td>
      <td>13:00:00</td>
    </tr>
    <tr>
      <th>2</th>
      <td>JEANNE2004</td>
      <td>2004-09-13</td>
      <td>2004-09-29</td>
      <td>2004-09-17</td>
      <td>18:00:00</td>
    </tr>
    <tr>
      <th>3</th>
      <td>DENNIS2005</td>
      <td>2005-07-04</td>
      <td>2005-07-18</td>
      <td>2005-07-07</td>
      <td>12:00:00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>EMILY2005</td>
      <td>2005-07-11</td>
      <td>2005-07-21</td>
      <td>2005-07-16</td>
      <td>05:00:00</td>
    </tr>
    <tr>
      <th>5</th>
      <td>STAN2005</td>
      <td>2005-10-01</td>
      <td>2005-10-05</td>
      <td>2005-10-01</td>
      <td>12:00:00</td>
    </tr>
    <tr>
      <th>6</th>
      <td>ALPHA2005</td>
      <td>2005-10-22</td>
      <td>2005-10-24</td>
      <td>2005-10-23</td>
      <td>11:20:00</td>
    </tr>
    <tr>
      <th>7</th>
      <td>ERNESTO2006</td>
      <td>2006-08-24</td>
      <td>2006-09-04</td>
      <td>2006-08-27</td>
      <td>23:00:00</td>
    </tr>
    <tr>
      <th>8</th>
      <td>DEAN2007</td>
      <td>2007-08-13</td>
      <td>2007-08-23</td>
      <td>2007-08-19</td>
      <td>12:00:00</td>
    </tr>
    <tr>
      <th>9</th>
      <td>NOEL2007</td>
      <td>2007-10-24</td>
      <td>2007-11-06</td>
      <td>2007-10-29</td>
      <td>08:20:00</td>
    </tr>
    <tr>
      <th>10</th>
      <td>OLGA2007</td>
      <td>2007-12-10</td>
      <td>2007-12-16</td>
      <td>2007-12-12</td>
      <td>06:00:00</td>
    </tr>
    <tr>
      <th>11</th>
      <td>FAY2008</td>
      <td>2008-08-15</td>
      <td>2008-08-28</td>
      <td>2008-08-16</td>
      <td>08:00:00</td>
    </tr>
    <tr>
      <th>12</th>
      <td>GUSTAV2008</td>
      <td>2008-08-25</td>
      <td>2008-09-05</td>
      <td>2008-08-26</td>
      <td>19:00:00</td>
    </tr>
    <tr>
      <th>13</th>
      <td>HANNA2008</td>
      <td>2008-08-28</td>
      <td>2008-09-08</td>
      <td>2008-09-03</td>
      <td>03:00:00</td>
    </tr>
    <tr>
      <th>14</th>
      <td>IKE2008</td>
      <td>2008-09-01</td>
      <td>2008-09-15</td>
      <td>2008-09-07</td>
      <td>12:00:00</td>
    </tr>
    <tr>
      <th>15</th>
      <td>TOMAS2010</td>
      <td>2010-10-29</td>
      <td>2010-11-10</td>
      <td>2010-11-05</td>
      <td>21:00:00</td>
    </tr>
    <tr>
      <th>16</th>
      <td>EMILY2011</td>
      <td>2011-08-02</td>
      <td>2011-08-07</td>
      <td>2011-08-04</td>
      <td>20:00:00</td>
    </tr>
    <tr>
      <th>17</th>
      <td>IRENE2011</td>
      <td>2011-08-21</td>
      <td>2011-08-30</td>
      <td>2011-08-23</td>
      <td>19:00:00</td>
    </tr>
    <tr>
      <th>18</th>
      <td>ISAAC2012</td>
      <td>2012-08-20</td>
      <td>2012-09-01</td>
      <td>2012-08-25</td>
      <td>06:00:00</td>
    </tr>
    <tr>
      <th>19</th>
      <td>SANDY2012</td>
      <td>2012-10-21</td>
      <td>2012-10-31</td>
      <td>2012-10-25</td>
      <td>02:00:00</td>
    </tr>
    <tr>
      <th>20</th>
      <td>ERIKA2015</td>
      <td>2015-08-24</td>
      <td>2015-08-28</td>
      <td>2015-08-28</td>
      <td>12:00:00</td>
    </tr>
    <tr>
      <th>21</th>
      <td>MATTHEW2016</td>
      <td>2016-09-28</td>
      <td>2016-10-10</td>
      <td>2016-10-04</td>
      <td>11:00:00</td>
    </tr>
    <tr>
      <th>22</th>
      <td>IRMA2017</td>
      <td>2017-08-30</td>
      <td>2017-09-13</td>
      <td>2017-09-08</td>
      <td>02:00:00</td>
    </tr>
    <tr>
      <th>23</th>
      <td>LAURA2020</td>
      <td>2020-08-20</td>
      <td>2020-08-29</td>
      <td>2020-08-23</td>
      <td>11:00:00</td>
    </tr>
    <tr>
      <th>24</th>
      <td>ELSA2021</td>
      <td>2021-06-30</td>
      <td>2021-07-10</td>
      <td>2021-07-03</td>
      <td>23:00:00</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Create the directory if it doesn't exist
os.makedirs(input_dir / '03_rainfall/input/', exist_ok=True)

# Save the DataFrame to CSV
df_metadata_fixed.to_csv(input_dir / '03_rainfall/input/metadata_typhoons.csv', index=False)
```
