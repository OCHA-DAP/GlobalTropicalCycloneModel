```python
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
import matplotlib.patches as mpatches
import numpy as np
import os
from pathlib import Path
import pandas as pd
import geopandas as gpd
from climada.hazard import Centroids, TCTracks, TropCyclone
from shapely.geometry import LineString
from sklearn.metrics import mean_squared_error
from xgboost.sklearn import XGBRegressor
import geopandas as gpd
from climada.hazard import Centroids, TCTracks, TropCyclone
from shapely.geometry import LineString
from xgboost import plot_importance
import shap
import ast

from utils import get_training_dataset_hti, get_combined_dataset, get_municipality_grids
```

    IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html


## Loading data


```python
# For Checking (Number of buildings destroyed per mun)
input_dir = (
    Path(os.getenv("STORM_DATA_DIR"))
    / "analysis_hti/02_model_features/02_housing_damage/input/"
)
# Grid dir
grid_dir = (
    Path(os.getenv("STORM_DATA_DIR"))
    / "analysis_hti/02_model_features/02_housing_damage/output/"
)

actual_mun_dmg = pd.read_csv(input_dir / "impact_data_bld_hti.csv")

# Load Fiji Shapefile
shp = gpd.read_file(
    input_dir / "shapefile_hti_fixed.gpkg"
)
shp = shp.to_crs('EPSG:4326')

# Load grid
grid = gpd.read_file(grid_dir / "hti_0.1_degree_grid_land_overlap.gpkg")


# Load typhoon track
intersection = actual_mun_dmg[['typhoon_name', 'Year', 'sid']].drop_duplicates().reset_index(drop=True)
```


```python
# Features
features= [
    "wind_speed",
    "track_distance",
    "total_houses",
    "rainfall_max_6h",
    "rainfall_max_24h",
    "coast_length",
    "with_coast",
    "mean_elev",
    "mean_slope",
    "IWI"
]
```


```python
# Load HTI + id
df_hti_complete = get_training_dataset_hti()
df_hti_complete = df_hti_complete.rename({
    "mean_altitude": "mean_elev",
    "total_buildings": "total_houses",
    "perc_dmg_grid_from_phl": "percent_houses_damaged"
}, axis=1)

# Load combined dataset
df_combined = get_combined_dataset(from_phl=True)
df_hti = df_combined[df_combined.country == 'hti']
```

## Stratification


```python
plt.hist(df_hti.percent_houses_damaged)
plt.yscale('log')
plt.xlabel('% of damage')
plt.title('HTI building damage by grid')
plt.show()
```



![png](02.1_combined_model_using_phl_rel_files/02.1_combined_model_using_phl_rel_6_0.png)




```python
# Stratification
dmg = np.array(df_hti.percent_houses_damaged.to_list())
zero_dmg = np.round((np.count_nonzero(dmg == 0) / len(dmg)) , 2 )

# Define ranges for each group
x0 = list(np.linspace(0, zero_dmg, 1))   # zero damage
x1 = list(np.linspace(zero_dmg, 0.9, 2))  # almost no damage
x2 = list(np.linspace(0.935, 1, 5))  # all the damage
x3=x0+x1+x2

bins = []
for i in x3:
    bins.append(np.quantile(dmg, i))

# Histogram after stratification
samples_per_bin, bins_def = np.histogram(dmg, bins=bins)

# Define number of bins
num_bins = len(samples_per_bin)

# For future plots
str_bin = []
for i in range(len(bins_def[:-1])):
    a = str(np.round(bins_def[i+1],3))
    b = str(np.round(bins_def[i],3))
    str_bin.append('{} - {}'.format(b,a))
print(str_bin)
```

    ['0.0 - 0.0', '0.0 - 0.0', '0.0 - 0.0', '0.0 - 0.001', '0.001 - 0.001', '0.001 - 0.003', '0.003 - 0.168']


## Define the model


```python
def xgb_model_combined_data_LOOCV(df_combined, df_hti, features, bins, hti_weight, fji_weight=1, phl_weight=1, viet_weight=1, combined=True):
    # Dataframe foir testing: HAITI
    hti_aux = df_hti[['typhoon_name', 'typhoon_year']].drop_duplicates()

    # Bins
    num_bins = len(bins)

    # The model
    rmse_total = []
    rmse_bin = []
    avg_error_bin = []

    y_test_typhoon  = []
    y_pred_typhoon  = []

    for typhoon, year in zip(hti_aux['typhoon_name'], hti_aux['typhoon_year']):

        """ PART 1: Train/Test """

        # LOOCV
        df_test = df_hti[
            (df_hti["typhoon_name"] == typhoon) &
            (df_hti["typhoon_year"] == year)] # Test set: HTI
        if combined:
            df_train = df_combined[
                (df_combined["typhoon_name"] != typhoon)
                #& (df_combined["typhoon_year"] != year) #I'm loosing information here... but its what it is for now
                ] # Train set: everything
            # Class weight
            weights = np.select(
                [
                    (df_train['country'] == 'phl'),
                    (df_train['country'] == 'viet'),
                    (df_train['country'] == 'fji'),
                    (df_train['country'] == 'hti')
                ],
                [
                    phl_weight,
                    viet_weight,
                    fji_weight,
                    hti_weight
                ],
                default=1
            )
        else:
            df_train = df_hti[
            (df_hti["typhoon_name"] != typhoon) &
            (df_hti["typhoon_year"] != year)] # Train set: HTI (everything else)

            df_train['country'] = 'hti'
            # Class weight
            weights = np.where(df_train['country'] == 'hti', 1, 1)

        # Split X and y from dataframe features
        X_test = df_test[features]
        X_train = df_train[features]

        y_train = df_train["percent_houses_damaged"]
        y_test = df_test["percent_houses_damaged"]

        # Stratify data
        bin_index_test = np.digitize(y_test, bins=bins[:-1])

        """ PART 2: XGB regressor """
        # create an XGBoost Regressor
        xgb = XGBRegressor(
            base_score=0.5,
            booster="gbtree",
            colsample_bylevel=0.8,
            colsample_bynode=0.8,
            colsample_bytree=0.8,
            gamma=3,
            eta=0.01,
            importance_type="gain",
            learning_rate=0.1,
            max_delta_step=0,
            max_depth=4,
            min_child_weight=1,
            missing=1,
            n_estimators=100,
            early_stopping_rounds=10,
            n_jobs=1,
            nthread=None,
            objective="reg:squarederror",
            reg_alpha=0,
            reg_lambda=1,
            scale_pos_weight=1,
            seed=None,
            silent=None,
            subsample=0.8,
            verbosity=0,
            eval_metric=["rmse", "logloss"],
            random_state=0,
        )

        # Fit the model
        eval_set = [(X_train, y_train)]
        xgb.fit(X_train, y_train, eval_set=eval_set, verbose=False, sample_weight=weights) #xgb_model

        # make predictions on Fiji
        y_pred = xgb.predict(X_test)

        # Save y_test y_pred
        y_test_typhoon.append(y_test)
        y_pred_typhoon.append(y_pred)

        # Calculate root mean squared error in total
        mse_test = mean_squared_error(y_test, y_pred)
        rmse_test = np.sqrt(mse_test)
        rmse_total.append(rmse_test)

        # Per bin (Stratification)
        rmse_test_bin = []
        avg_error_bin = []
        for bin_num in range(num_bins)[1:]:
            if (len(y_test[bin_index_test == bin_num]) != 0 and len(y_pred[bin_index_test == bin_num]) != 0):
                # Estimation of RMSE for test data per each bin
                mse_test = mean_squared_error(y_test[bin_index_test == bin_num], y_pred[bin_index_test == bin_num])
                rmse_test = np.sqrt(mse_test)
                rmse_test_bin.append(rmse_test)
                # Avg error
                mean_difference = np.mean(y_test[bin_index_test == bin_num] - y_pred[bin_index_test == bin_num])
                avg_error_bin.append(mean_difference)
            else:
                rmse_test_bin.append(np.nan)
                avg_error_bin.append(np.nan)

        rmse_bin.append(rmse_test_bin)
        avg_error_bin.append(avg_error_bin)

        # RMSE & Avg error per bin
        rmse_strat = []
        avg_error_strat = []
        for i in range(num_bins - 1):
            #RMSE
            test_rmse_bin = np.nanmean(np.array(rmse_bin)[:,i])
            rmse_strat.append(test_rmse_bin)
            # #AVG error
            # test_avg_bin = np.nanmean(np.array(avg_error_bin)[:,i])
            # avg_error_strat.append(test_avg_bin)

    return y_test_typhoon, y_pred_typhoon, rmse_strat, rmse_total
```

## The model


```python
# Fji + Phl + Viet + Hti
y_test_typhoon_hti, y_pred_typhoon_hti, rmse_strat, rmse_total = xgb_model_combined_data_LOOCV(
    df_combined=df_combined,
    df_hti=df_hti_complete,
    bins=bins_def,
    hti_weight=4,
    features=features
)
```

    Mean of empty slice
    Mean of empty slice



```python
# Fji + Phl + Viet + Hti --> trained on just HTI data
y_test_typhoon_hti_justhti, y_pred_typhoon_hti_justhti, rmse_strat_justhti, rmse_total_justhti = xgb_model_combined_data_LOOCV(
    df_combined=df_combined,
    df_hti=df_hti_complete,
    bins=bins_def,
    hti_weight=4,
    features=features,
    combined=False
)
```

    Mean of empty slice
    Mean of empty slice


## Results per bin


```python
fig, ax = plt.subplots(1,1, figsize=(6,6))

ax.plot(range(num_bins), rmse_strat, 'rs', alpha=0.5, label='Combined model, weight ratio: 4:1')
ax.plot(range(num_bins), rmse_strat_justhti, 'bs', alpha=0.5, label='Model trained on HTI data')

ax.set_xticks(range(num_bins), str_bin, rotation=45)
ax.set_xlabel('Damage [%]')
ax.set_ylabel('RMSE')
ax.grid()
ax.set_title('XGBoost Regression model (HTI+PHL+VIET+FJI) \n using LOOCV for Haiti TCs')
ax.legend()

plt.tight_layout()
plt.show()
```



![png](02.1_combined_model_using_phl_rel_files/02.1_combined_model_using_phl_rel_14_0.png)



## Results at municipality level


```python
# typhoons
hti_typhoons = df_hti_complete.typhoon_name.unique()

# Calculate buildings destroyed by municipality and % of buildings destroyed by mun
mun_id = get_municipality_grids()[['id','ADM1_PCODE']].drop_duplicates()

def num_bld_destroyed_mun(mun, typhoon, year, y_pred_typhoon, df_hti, real=False):
    k = hti_typhoons.tolist().index(typhoon)
    df_typhoon = df_hti[(df_hti.typhoon_name==typhoon) & (df_hti.typhoon_year==year)]
    # Add feature "predictive_damage"
    df_typhoon['predicted_damage'] = y_pred_typhoon[k]

    mun_ids = mun_id[mun_id.ADM1_PCODE == mun].id.to_list()
    cells_in_mun = df_typhoon[df_typhoon.typhoon_name == typhoon].set_index('grid_point_id').loc[mun_ids]

    if real:
        damage_grid = np.array(cells_in_mun.percent_houses_damaged.to_list()) # Real dmg
    else:
        damage_grid = np.array(cells_in_mun.predicted_damage.to_list()) # Dmg predicted by cell

    # Number of buildings
    N_bld_grid = np.array(cells_in_mun.total_houses.to_list()) # Bld by cell
    N_bld_mun = np.sum(N_bld_grid) # Total bld in mun

    # Calculate % of buildings (and N of bld) destroyed by mun
    N_bld_dest_pred_mun = np.sum(damage_grid) * (N_bld_mun / 100)
    perc_destroyed_mun = np.sum(damage_grid)

    return N_bld_dest_pred_mun, perc_destroyed_mun

def calculate_actual_perc_dmg(x, i, y_pred_typhoon, df_fji, typhoon, year):
    try:
        return num_bld_destroyed_mun(mun=x['ADM1_PCODE'], typhoon=typhoon, year=year, y_pred_typhoon=y_pred_typhoon, df_fji=df_fji, real=True)[i]
    except:
        return 0
def calculate_pred_perc_dmg(x, i, y_pred_typhoon, df_fji, typhoon, year):
    try:
        return num_bld_destroyed_mun(mun=x['ADM1_PCODE'], typhoon=typhoon, year=year, y_pred_typhoon=y_pred_typhoon, df_fji=df_fji, real=False)[i]
    except:
        return 0
```


```python
# Good example
typhoon='JEANNE'
year=2004
mun = 'HT05'

num_bld_destroyed_mun(mun=mun, typhoon=typhoon,
                      year=year, y_pred_typhoon=y_pred_typhoon_hti, df_hti=df_hti_complete, real=True)
```




    (362.71880680824677, 0.05650124879016128)



### Specific cases


```python
# Good example
typhoon='JEANNE'
year=2004
```


```python
shp_reduced = shp[['ADM1_PCODE', 'geometry']].copy()
# Apply the function for real percentage damage
result_actual = shp_reduced.apply(lambda row: num_bld_destroyed_mun(mun=row['ADM1_PCODE'],
                                                            typhoon=typhoon,
                                                            year=year,
                                                            y_pred_typhoon=y_pred_typhoon_hti,
                                                            df_hti=df_hti_complete,
                                                            real=True), axis=1)

# Apply the function for predicted percentage damage
result_pred = shp_reduced.apply(lambda row: num_bld_destroyed_mun(mun=row['ADM1_PCODE'],
                                                          typhoon=typhoon,
                                                          year=year,
                                                          y_pred_typhoon=y_pred_typhoon_hti,
                                                          df_hti=df_hti_complete,
                                                          real=False), axis=1)

# Unpack the resulting tuples into separate columns for actual and predicted percentage damage
shp_reduced['actual_N_bld_dest_mun'], shp_reduced['actual_perc_dmg'] = zip(*result_actual)
shp_reduced['pred_N_bld_dest_mun'], shp_reduced['pred_perc_dmg'] = zip(*result_pred)

# Modify values
shp_reduced['actual_perc_dmg'] = shp_reduced['actual_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
shp_reduced['pred_perc_dmg'] = shp_reduced['pred_perc_dmg'].apply(lambda x: 0 if x < 0 else (100 if x > 100 else x))
# Now calculate prediciton error
shp_reduced['prediction_error'] = shp_reduced['actual_perc_dmg'] - shp_reduced['pred_perc_dmg'] # in percentual points

```


```python
# Load track
id = intersection[intersection['typhoon_name'] == typhoon].sid.to_list()
track = TCTracks.from_ibtracs_netcdf(storm_id=id)
tc_track = track.get_track()

points_ib = gpd.points_from_xy(tc_track.lon, tc_track.lat)
tc_track_line_ib = LineString(points_ib)

geometries_ib = gpd.GeoSeries([tc_track_line_ib])
line_gdf_ib = gpd.GeoDataFrame(geometry=geometries_ib)

# Plots
cmap='Reds'
cmap_blue = 'Blues'
cmap_red = 'Reds_r'
```

    2024-04-10 18:02:57,070 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /Users/federico/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.



```python
fig, ax = plt.subplots(1, 3, figsize=(15, 5))
ax = ax.flatten()
# Check prediction_error column for values > 0 and < 0
positive_error = shp_reduced[shp_reduced['prediction_error'] >= 0]
negative_error = shp_reduced[shp_reduced['prediction_error'] < 0]


# Plotting the maps
fiji_plot_1 = shp_reduced.plot(column='actual_perc_dmg', cmap=cmap, linewidth=0.2, ax=ax[0], edgecolor='0.3', legend=True)
fiji_plot_2 = shp_reduced.plot(column='pred_perc_dmg', cmap=cmap, linewidth=0.2, ax=ax[1], edgecolor='0.3', legend=True)
fiji_plot_3_blue = positive_error.plot(column='prediction_error', cmap=cmap_blue, linewidth=0.2, ax=ax[2], edgecolor='0.3', vmin=0, legend=True)
fiji_plot_3_red = negative_error.plot(column='prediction_error', cmap=cmap_red, linewidth=0.2, ax=ax[2], edgecolor='0.3', vmax=0, legend=True)


line_gdf_ib.plot(ax=ax[0], color='k', linewidth=1, label='Typhoon track')  # Plot the LineString in black
line_gdf_ib.plot(ax=ax[1], color='k', linewidth=1, label='Typhoon track')  # Plot the LineString in black
line_gdf_ib.plot(ax=ax[2], color='k', linewidth=1, label='Typhoon track')  # Plot the LineString in black


# Create custom legends
blue_patch = mpatches.Patch(color='#6495ED', label='Underestimated damage')
red_patch = mpatches.Patch(color='#800000', label='Overestimated damage')
ax[2].legend(handles=[blue_patch, red_patch], loc='lower left', bbox_to_anchor=(-0.05, -0.2))

ax[0].set_title('Actual Damage by department (ADM1)')
ax[1].set_title('Predicted Damage by department (ADM1)')
ax[2].set_title('Prediction Error \n $actual_{dmg} - predicted_{dmg}$ \n(in percentage points)', y=0.95)

ax[0].axis('off')
ax[1].axis('off')
ax[2].axis('off')


# All shp map
ax[0].set_xlim([-76, -70])
ax[0].set_ylim([17, 22])
ax[1].set_xlim([-76, -70])
ax[1].set_ylim([17, 22])
ax[2].set_xlim([-76, -70])
ax[2].set_ylim([17, 22])

plt.suptitle("Typhoon {} HTI+PHL+FJI+VIET XGBoost model (HTI-rest 4:1 ratio)".format(typhoon))
plt.tight_layout()
plt.show()
```



![png](02.1_combined_model_using_phl_rel_files/02.1_combined_model_using_phl_rel_22_0.png)




```python
actual_dmg = shp_reduced.drop_duplicates(subset='ADM1_PCODE')['actual_perc_dmg'].reset_index(drop=True)
pred_dmg = shp_reduced.drop_duplicates(subset='ADM1_PCODE')['pred_perc_dmg'].reset_index(drop=True)
error_baseline = actual_dmg - actual_dmg
error_old = actual_dmg - pred_dmg
muns = shp_reduced.drop_duplicates(subset='ADM1_PCODE')['ADM1_PCODE'].reset_index(drop=True)
```


```python
plt.plot(error_baseline, 'o-',label='Perfect model', alpha=1)
plt.plot(error_old, 'o-', label='HTI-rest 4:1 ratio', alpha=0.4)


plt.xticks(range(len(muns)), muns, rotation='vertical')  # Set x-axis ticks and labels
plt.tight_layout()  # Adjust layout to prevent overlapping
plt.title('Tropical Cyclone: {}, HTI-PHL+FJI+VNM model'.format(typhoon))
plt.ylabel('% Points of damage \n difference')

plt.grid()
plt.legend(loc='upper left')
plt.show()
```



![png](02.1_combined_model_using_phl_rel_files/02.1_combined_model_using_phl_rel_24_0.png)



## ADM0 aggregation


```python
df_hti_complete.columns
```




    Index(['index', 'typhoon_name', 'typhoon_year', 'event_level', 'grid_point_id',
           'total_buildings_damaged', 'total_buildings_damaged_from_phl',
           'total_pop_affected', 'total_houses', 'total_pop', 'perc_dmg_grid',
           'percent_houses_damaged', 'perc_aff_pop_grid', 'track_id', 'wind_speed',
           'track_distance', 'rainfall_max_6h', 'rainfall_max_24h', 'with_coast',
           'coast_length', 'mean_elev', 'mean_slope', 'IWI'],
          dtype='object')




```python
# typhoons
hti_typhoons = df_hti.typhoon_name.unique()

# Calculate buildings destroyed by municipality and % of buildings destroyed by mun
mun_id_adm0 = get_municipality_grids()[['id','ADM1_PCODE']].drop_duplicates()
mun_id_adm0['ADM0_PCODE'] = 'HT'

def num_bld_aff_adm0(typhoon, year, y_pred_typhoon, df_hti, real=False):
    k = hti_typhoons.tolist().index(typhoon)
    df_typhoon = df_hti[(df_hti.typhoon_name==typhoon) & (df_hti.typhoon_year==year)]
    # Add feature "predictive_damage"
    df_typhoon['predicted_damage'] = y_pred_typhoon[k]

    mun_ids = mun_id_adm0[mun_id_adm0.ADM0_PCODE == 'HT'].id.to_list() # All
    cells_in_mun = df_typhoon[df_typhoon.typhoon_name == typhoon].set_index('grid_point_id').loc[mun_ids]

    if real:
        damage_grid = np.array(cells_in_mun.percent_houses_damaged.to_list()) # Real dmg
    else:
        damage_grid = np.array(cells_in_mun.predicted_damage.to_list()) # Dmg predicted by cell

    # Number of buildings
    N_bld_grid = np.array(cells_in_mun.total_houses.to_list()) # Bld by cell
    N_bld_mun = np.sum(N_bld_grid) # Total bld in mun

    # Calculate % of buildings (and N of bld) destroyed by mun
    N_bld_dest_pred_mun = np.sum(damage_grid) * (N_bld_mun / 100)
    perc_destroyed_mun = np.sum(damage_grid)

    return N_bld_dest_pred_mun, perc_destroyed_mun

```


```python
df_hti_aux = df_hti_complete[['typhoon_name', 'typhoon_year']].drop_duplicates()
actual_damage_typhoons = []
pred_damage_typhoons = []
actual_N_damage_typhoons = []
pred_N_damage_typhoons = []
output_df = pd.DataFrame()

for typhoon, year in zip(df_hti_aux.typhoon_name, df_hti_aux.typhoon_year):
    actual_N_dmg, actual_dmg = num_bld_aff_adm0(typhoon=typhoon,
                                    year=year,
                                    y_pred_typhoon=y_pred_typhoon_hti,
                                    df_hti=df_hti_complete,
                                    real=True)
    pred_N_dmg, pred_dmg = num_bld_aff_adm0(typhoon=typhoon,
                                    year=year,
                                    y_pred_typhoon=y_pred_typhoon_hti,
                                    df_hti=df_hti_complete,
                                    real=False)
    df2append = pd.DataFrame({'event':[typhoon],
                   'year':[year],
                   'bld_affected_adm0':[actual_N_dmg],
                   'prediction_adm0':[pred_N_dmg]})
    output_df = pd.concat([output_df, df2append])
    # actual_damage_typhoons.append(actual_dmg)
    # actual_N_damage_typhoons.append(actual_N_dmg)
    # pred_damage_typhoons.append(pred_dmg)
    # pred_N_damage_typhoons.append(pred_N_dmg)
```


```python
typhoons= df_hti_aux.typhoon_name.to_list()
years = df_hti_aux.typhoon_year.to_list()

# Create a copy of the original dataframe
output_aux = output_df.copy()

# Replace prediction_adm0 values less than 0 with 1 in the copied dataframe
output_aux.loc[output_aux['prediction_adm0'] < 0, 'prediction_adm0'] = 1
actual_N_damage_typhoons = output_aux['bld_affected_adm0']
pred_N_damage_typhoons = output_aux['prediction_adm0']
```


```python
plt.plot(actual_N_damage_typhoons, pred_N_damage_typhoons, 'o', label='Haiti events')
plt.xlabel('Actual affected bld')
plt.ylabel('Predicted affected bld')

# Calculate the range for the 45-degree line
min_val = 0 # min(min(actual_N_damage_typhoons), min(pred_N_damage_typhoons))
max_val = max(max(actual_N_damage_typhoons), max(pred_N_damage_typhoons))
line_range = np.linspace(min_val, max_val, 100)

# Plot the 45-degree line within the range
plt.plot(line_range, line_range, 'r-', label='Perfect model')
plt.title('Number of affected buildings at ADM0 level for every TC')

plt.legend()
plt.grid()
plt.show()
```



![png](02.1_combined_model_using_phl_rel_files/02.1_combined_model_using_phl_rel_30_0.png)




```python
plt.plot(actual_N_damage_typhoons, pred_N_damage_typhoons, 'o', label='Haiti events')
plt.xlabel('Actual affected bld')
plt.ylabel('Predicted affected bld')

# # Add text annotations for each point
# for i, (x, y, typhoon, year) in enumerate(zip(actual_damage_typhoons, pred_damage_typhoons, typhoons, years)):
#     plt.text(x, y, f'{typhoon}', fontsize=6, ha='left', va='center')

# Calculate the range for the 45-degree line
min_val = 0 # min(min(actual_N_damage_typhoons), min(pred_N_damage_typhoons))
max_val = max(max(actual_N_damage_typhoons), max(pred_N_damage_typhoons))
line_range = np.linspace(min_val, max_val, 100)

# Plot the 45-degree line within the range
plt.plot(line_range, line_range, 'r-', label='Perfect model')
plt.title('Number of affected buildings at ADM0 level for every TC')
plt.xscale('log')
plt.yscale('log')

plt.legend()
plt.grid()
plt.show()
```



![png](02.1_combined_model_using_phl_rel_files/02.1_combined_model_using_phl_rel_31_0.png)



Which are these events?


```python
subset_strange = output_aux[output_aux.prediction_adm0 == 1].rename({
        'event':'typhoon_name',
        'year':'typhoon_year'
    }, axis=1)
df_events = df_hti_complete[['typhoon_name', 'typhoon_year', 'event_level']].drop_duplicates()
subset_strange.merge(df_events)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>typhoon_name</th>
      <th>typhoon_year</th>
      <th>bld_affected_adm0</th>
      <th>prediction_adm0</th>
      <th>event_level</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>LILI</td>
      <td>2002</td>
      <td>566.935811</td>
      <td>1.0</td>
      <td>ADM1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>IVAN</td>
      <td>2004</td>
      <td>36.112875</td>
      <td>1.0</td>
      <td>ADM2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>DENNIS</td>
      <td>2005</td>
      <td>710.314960</td>
      <td>1.0</td>
      <td>ADM1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>EMILY</td>
      <td>2005</td>
      <td>16.611687</td>
      <td>1.0</td>
      <td>ADM2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>STAN</td>
      <td>2005</td>
      <td>41.203996</td>
      <td>1.0</td>
      <td>ADM2</td>
    </tr>
    <tr>
      <th>5</th>
      <td>ALPHA</td>
      <td>2005</td>
      <td>97.964683</td>
      <td>1.0</td>
      <td>ADM2</td>
    </tr>
    <tr>
      <th>6</th>
      <td>ERNESTO</td>
      <td>2006</td>
      <td>830.536953</td>
      <td>1.0</td>
      <td>ADM1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>OLGA</td>
      <td>2007</td>
      <td>116.006577</td>
      <td>1.0</td>
      <td>ADM1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>FAY</td>
      <td>2008</td>
      <td>565.611212</td>
      <td>1.0</td>
      <td>ADM1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>EMILY</td>
      <td>2011</td>
      <td>49.100106</td>
      <td>1.0</td>
      <td>ADM1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>IRENE</td>
      <td>2011</td>
      <td>50.366164</td>
      <td>1.0</td>
      <td>ADM1</td>
    </tr>
    <tr>
      <th>11</th>
      <td>ISAAC</td>
      <td>2012</td>
      <td>656.263610</td>
      <td>1.0</td>
      <td>ADM1</td>
    </tr>
    <tr>
      <th>12</th>
      <td>ERIKA</td>
      <td>2015</td>
      <td>110.325302</td>
      <td>1.0</td>
      <td>ADM2</td>
    </tr>
    <tr>
      <th>13</th>
      <td>MATTHEW</td>
      <td>2016</td>
      <td>57974.631689</td>
      <td>1.0</td>
      <td>ADM1</td>
    </tr>
  </tbody>
</table>
</div>
